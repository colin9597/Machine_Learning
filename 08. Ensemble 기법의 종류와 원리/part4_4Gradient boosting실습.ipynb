{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\포스코 아카데미\\\\새 폴더\\\\전체강의자료\\\\Part 05~11) Machine Learning\\\\08. Ensemble 기법의 종류와 원리\\\\실습코드'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재경로 확인\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
       "0   1       1       0       0       0       0       0       0       0       0   \n",
       "1   2       0       0       0       0       0       0       0       1       0   \n",
       "2   3       0       0       0       0       0       0       0       1       0   \n",
       "3   4       1       0       0       1       6       1       5       0       0   \n",
       "4   5       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   ...  feat_85  feat_86  feat_87  feat_88  feat_89  feat_90  feat_91  \\\n",
       "0  ...        1        0        0        0        0        0        0   \n",
       "1  ...        0        0        0        0        0        0        0   \n",
       "2  ...        0        0        0        0        0        0        0   \n",
       "3  ...        0        1        2        0        0        0        0   \n",
       "4  ...        1        0        0        0        0        1        0   \n",
       "\n",
       "   feat_92  feat_93   target  \n",
       "0        0        0  Class_1  \n",
       "1        0        0  Class_1  \n",
       "2        0        0  Class_1  \n",
       "3        0        0  Class_1  \n",
       "4        0        0  Class_1  \n",
       "\n",
       "[5 rows x 95 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "data = pd.read_csv(\"../Data/otto_train.csv\") # Product Category\n",
    "data.head() # 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid: 고유 아이디\\nfeat_1 ~ feat_93: 설명변수\\ntarget: 타겟변수 (1~9)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "id: 고유 아이디\n",
    "feat_1 ~ feat_93: 설명변수\n",
    "target: 타겟변수 (1~9)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nCar: 61878 nVar: 95\n"
     ]
    }
   ],
   "source": [
    "nCar = data.shape[0] # 데이터 개수\n",
    "nVar = data.shape[1] # 변수 개수\n",
    "print('nCar: %d' % nCar, 'nVar: %d' % nVar )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 의미가 없다고 판단되는 변수 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['id'], axis = 1) # id 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 타겟 변수의 문자열을 숫자로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {\"Class_1\": 1,\n",
    "                \"Class_2\": 2,\n",
    "                \"Class_3\": 3,\n",
    "                \"Class_4\": 4,\n",
    "                \"Class_5\": 5,\n",
    "                \"Class_6\": 6,\n",
    "                \"Class_7\": 7,\n",
    "                \"Class_8\": 8,\n",
    "                \"Class_9\": 9}\n",
    "after_mapping_target = data['target'].apply(lambda x: mapping_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설명변수와 타겟변수를 분리, 학습데이터와 평가데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49502, 93) (12376, 93) (49502,) (12376,)\n"
     ]
    }
   ],
   "source": [
    "feature_columns = list(data.columns.difference(['target'])) # target을 제외한 모든 행\n",
    "X = data[feature_columns] # 설명변수\n",
    "y = after_mapping_target # 타겟변수\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 42) # 학습데이터와 평가데이터의 비율을 8:2 로 분할| \n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape) # 데이터 개수 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:12:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:12:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 76.67 %\n",
      "Time: 4.67 seconds\n"
     ]
    }
   ],
   "source": [
    "# !pip install xgboost\n",
    "import xgboost as xgb\n",
    "import time\n",
    "start = time.time() # 시작 시간 지정\n",
    "xgb_dtrain = xgb.DMatrix(data = train_x, label = train_y) # 학습 데이터를 XGBoost 모델에 맞게 변환\n",
    "xgb_dtest = xgb.DMatrix(data = test_x) # 평가 데이터를 XGBoost 모델에 맞게 변환\n",
    "xgb_param = {'max_depth': 10, # 트리 깊이\n",
    "         'learning_rate': 0.01, # Step Size\n",
    "         'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "         'objective': 'multi:softmax', # 목적 함수\n",
    "        'num_class': len(set(train_y)) + 1} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "xgb_model = xgb.train(params = xgb_param, dtrain = xgb_dtrain) # 학습 진행\n",
    "xgb_model_predict = xgb_model.predict(xgb_dtest) # 평가 데이터 예측\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, xgb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 3., 6., ..., 9., 2., 7.], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008106 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3110\n",
      "[LightGBM] [Info] Number of data points in the train set: 49502, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -3.476745\n",
      "[LightGBM] [Info] Start training from score -1.341381\n",
      "[LightGBM] [Info] Start training from score -2.039019\n",
      "[LightGBM] [Info] Start training from score -3.135151\n",
      "[LightGBM] [Info] Start training from score -3.125444\n",
      "[LightGBM] [Info] Start training from score -1.481556\n",
      "[LightGBM] [Info] Start training from score -3.074772\n",
      "[LightGBM] [Info] Start training from score -1.986562\n",
      "[LightGBM] [Info] Start training from score -2.533374\n",
      "Accuracy: 76.28 %\n",
      "Time: 1.92 seconds\n"
     ]
    }
   ],
   "source": [
    "# !pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "start = time.time() # 시작 시간 지정\n",
    "lgb_dtrain = lgb.Dataset(data = train_x, label = train_y) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "lgb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "            'objective': 'multiclass', # 목적 함수\n",
    "            'num_class': len(set(train_y)) + 1} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n",
    "lgb_model_predict = np.argmax(lgb_model.predict(test_x), axis = 1) # 평가 데이터 예측, Softmax의 결과값 중 가장 큰 값의 Label로 예측\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, lgb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.01734061e-15, 2.25081693e-02, 3.62193933e-01, ...,\n",
       "        3.24234521e-02, 5.82126692e-02, 3.67722414e-02],\n",
       "       [1.14084116e-15, 5.36978636e-02, 1.90687128e-01, ...,\n",
       "        3.25081119e-01, 9.38028846e-02, 6.50463131e-02],\n",
       "       [5.94595781e-16, 9.66842220e-03, 5.82817482e-02, ...,\n",
       "        1.42318289e-02, 3.40230275e-02, 2.14919364e-02],\n",
       "       ...,\n",
       "       [7.09105769e-16, 4.63740004e-02, 1.08297559e-01, ...,\n",
       "        5.46934960e-02, 7.24513712e-02, 5.74635996e-01],\n",
       "       [9.88127136e-16, 1.54895684e-02, 5.45515599e-01, ...,\n",
       "        2.45870954e-02, 5.65410617e-02, 3.62344513e-02],\n",
       "       [7.59617500e-16, 1.49480877e-02, 7.44570300e-02, ...,\n",
       "        5.76695793e-01, 1.43227106e-01, 2.74567219e-02]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5907034\ttotal: 676ms\tremaining: 1m 6s\n",
      "1:\tlearn: 0.6356107\ttotal: 1.23s\tremaining: 1m\n",
      "2:\tlearn: 0.6411256\ttotal: 1.81s\tremaining: 58.4s\n",
      "3:\tlearn: 0.6480344\ttotal: 2.37s\tremaining: 56.8s\n",
      "4:\tlearn: 0.6508222\ttotal: 2.93s\tremaining: 55.6s\n",
      "5:\tlearn: 0.6499939\ttotal: 3.59s\tremaining: 56.2s\n",
      "6:\tlearn: 0.6507818\ttotal: 4.19s\tremaining: 55.7s\n",
      "7:\tlearn: 0.6548422\ttotal: 4.75s\tremaining: 54.7s\n",
      "8:\tlearn: 0.6559533\ttotal: 5.3s\tremaining: 53.6s\n",
      "9:\tlearn: 0.6560947\ttotal: 5.85s\tremaining: 52.6s\n",
      "10:\tlearn: 0.6568421\ttotal: 6.42s\tremaining: 52s\n",
      "11:\tlearn: 0.6588219\ttotal: 7.03s\tremaining: 51.5s\n",
      "12:\tlearn: 0.6592259\ttotal: 7.59s\tremaining: 50.8s\n",
      "13:\tlearn: 0.6611248\ttotal: 8.14s\tremaining: 50s\n",
      "14:\tlearn: 0.6625591\ttotal: 8.69s\tremaining: 49.3s\n",
      "15:\tlearn: 0.6631853\ttotal: 9.25s\tremaining: 48.6s\n",
      "16:\tlearn: 0.6639328\ttotal: 9.8s\tremaining: 47.8s\n",
      "17:\tlearn: 0.6668821\ttotal: 10.3s\tremaining: 47.1s\n",
      "18:\tlearn: 0.6669630\ttotal: 10.9s\tremaining: 46.5s\n",
      "19:\tlearn: 0.6675286\ttotal: 11.4s\tremaining: 45.8s\n",
      "20:\tlearn: 0.6673266\ttotal: 12s\tremaining: 45.1s\n",
      "21:\tlearn: 0.6677104\ttotal: 12.6s\tremaining: 44.6s\n",
      "22:\tlearn: 0.6682558\ttotal: 13.1s\tremaining: 44s\n",
      "23:\tlearn: 0.6683972\ttotal: 13.7s\tremaining: 43.3s\n",
      "24:\tlearn: 0.6686599\ttotal: 14.2s\tremaining: 42.6s\n",
      "25:\tlearn: 0.6681952\ttotal: 14.8s\tremaining: 42s\n",
      "26:\tlearn: 0.6684982\ttotal: 15.3s\tremaining: 41.3s\n",
      "27:\tlearn: 0.6692053\ttotal: 15.8s\tremaining: 40.7s\n",
      "28:\tlearn: 0.6696699\ttotal: 16.4s\tremaining: 40.1s\n",
      "29:\tlearn: 0.6699325\ttotal: 16.9s\tremaining: 39.5s\n",
      "30:\tlearn: 0.6705992\ttotal: 17.4s\tremaining: 38.8s\n",
      "31:\tlearn: 0.6709426\ttotal: 18s\tremaining: 38.2s\n",
      "32:\tlearn: 0.6708012\ttotal: 18.5s\tremaining: 37.6s\n",
      "33:\tlearn: 0.6709426\ttotal: 19s\tremaining: 37s\n",
      "34:\tlearn: 0.6707002\ttotal: 19.6s\tremaining: 36.4s\n",
      "35:\tlearn: 0.6715082\ttotal: 20.1s\tremaining: 35.7s\n",
      "36:\tlearn: 0.6705992\ttotal: 20.6s\tremaining: 35.1s\n",
      "37:\tlearn: 0.6725991\ttotal: 21.1s\tremaining: 34.5s\n",
      "38:\tlearn: 0.6729829\ttotal: 21.7s\tremaining: 33.9s\n",
      "39:\tlearn: 0.6725991\ttotal: 22.2s\tremaining: 33.3s\n",
      "40:\tlearn: 0.6734273\ttotal: 22.7s\tremaining: 32.7s\n",
      "41:\tlearn: 0.6738314\ttotal: 23.2s\tremaining: 32.1s\n",
      "42:\tlearn: 0.6741546\ttotal: 23.8s\tremaining: 31.5s\n",
      "43:\tlearn: 0.6739728\ttotal: 24.4s\tremaining: 31s\n",
      "44:\tlearn: 0.6741950\ttotal: 24.9s\tremaining: 30.5s\n",
      "45:\tlearn: 0.6750636\ttotal: 25.5s\tremaining: 29.9s\n",
      "46:\tlearn: 0.6758919\ttotal: 26s\tremaining: 29.3s\n",
      "47:\tlearn: 0.6757707\ttotal: 26.6s\tremaining: 28.8s\n",
      "48:\tlearn: 0.6762151\ttotal: 27.2s\tremaining: 28.3s\n",
      "49:\tlearn: 0.6774474\ttotal: 27.8s\tremaining: 27.8s\n",
      "50:\tlearn: 0.6777100\ttotal: 28.4s\tremaining: 27.2s\n",
      "51:\tlearn: 0.6786594\ttotal: 28.9s\tremaining: 26.7s\n",
      "52:\tlearn: 0.6789827\ttotal: 29.4s\tremaining: 26.1s\n",
      "53:\tlearn: 0.6804372\ttotal: 30s\tremaining: 25.6s\n",
      "54:\tlearn: 0.6804372\ttotal: 30.7s\tremaining: 25.1s\n",
      "55:\tlearn: 0.6809220\ttotal: 31.4s\tremaining: 24.6s\n",
      "56:\tlearn: 0.6812250\ttotal: 32s\tremaining: 24.2s\n",
      "57:\tlearn: 0.6813058\ttotal: 32.6s\tremaining: 23.6s\n",
      "58:\tlearn: 0.6811846\ttotal: 33.3s\tremaining: 23.1s\n",
      "59:\tlearn: 0.6813260\ttotal: 33.9s\tremaining: 22.6s\n",
      "60:\tlearn: 0.6816694\ttotal: 34.4s\tremaining: 22s\n",
      "61:\tlearn: 0.6823159\ttotal: 35s\tremaining: 21.5s\n",
      "62:\tlearn: 0.6832653\ttotal: 35.6s\tremaining: 20.9s\n",
      "63:\tlearn: 0.6840734\ttotal: 36.3s\tremaining: 20.4s\n",
      "64:\tlearn: 0.6840734\ttotal: 37s\tremaining: 19.9s\n",
      "65:\tlearn: 0.6846592\ttotal: 37.6s\tremaining: 19.4s\n",
      "66:\tlearn: 0.6843360\ttotal: 38.2s\tremaining: 18.8s\n",
      "67:\tlearn: 0.6846390\ttotal: 38.8s\tremaining: 18.2s\n",
      "68:\tlearn: 0.6854269\ttotal: 39.4s\tremaining: 17.7s\n",
      "69:\tlearn: 0.6858309\ttotal: 40.1s\tremaining: 17.2s\n",
      "70:\tlearn: 0.6858309\ttotal: 40.7s\tremaining: 16.6s\n",
      "71:\tlearn: 0.6865783\ttotal: 41.3s\tremaining: 16s\n",
      "72:\tlearn: 0.6864167\ttotal: 41.9s\tremaining: 15.5s\n",
      "73:\tlearn: 0.6868611\ttotal: 42.4s\tremaining: 14.9s\n",
      "74:\tlearn: 0.6869217\ttotal: 43s\tremaining: 14.3s\n",
      "75:\tlearn: 0.6870429\ttotal: 43.5s\tremaining: 13.7s\n",
      "76:\tlearn: 0.6875278\ttotal: 44.1s\tremaining: 13.2s\n",
      "77:\tlearn: 0.6881136\ttotal: 44.6s\tremaining: 12.6s\n",
      "78:\tlearn: 0.6883762\ttotal: 45.2s\tremaining: 12s\n",
      "79:\tlearn: 0.6888207\ttotal: 45.8s\tremaining: 11.4s\n",
      "80:\tlearn: 0.6892449\ttotal: 46.3s\tremaining: 10.9s\n",
      "81:\tlearn: 0.6898509\ttotal: 46.9s\tremaining: 10.3s\n",
      "82:\tlearn: 0.6897095\ttotal: 47.4s\tremaining: 9.71s\n",
      "83:\tlearn: 0.6902549\ttotal: 48s\tremaining: 9.14s\n",
      "84:\tlearn: 0.6909822\ttotal: 48.6s\tremaining: 8.57s\n",
      "85:\tlearn: 0.6910832\ttotal: 49.1s\tremaining: 7.99s\n",
      "86:\tlearn: 0.6914468\ttotal: 49.7s\tremaining: 7.42s\n",
      "87:\tlearn: 0.6916084\ttotal: 50.2s\tremaining: 6.85s\n",
      "88:\tlearn: 0.6919922\ttotal: 50.8s\tremaining: 6.28s\n",
      "89:\tlearn: 0.6925579\ttotal: 51.4s\tremaining: 5.71s\n",
      "90:\tlearn: 0.6928407\ttotal: 52s\tremaining: 5.14s\n",
      "91:\tlearn: 0.6930427\ttotal: 52.6s\tremaining: 4.57s\n",
      "92:\tlearn: 0.6935073\ttotal: 53.1s\tremaining: 4s\n",
      "93:\tlearn: 0.6940932\ttotal: 53.7s\tremaining: 3.43s\n",
      "94:\tlearn: 0.6944972\ttotal: 54.3s\tremaining: 2.86s\n",
      "95:\tlearn: 0.6948810\ttotal: 54.8s\tremaining: 2.28s\n",
      "96:\tlearn: 0.6951840\ttotal: 55.4s\tremaining: 1.71s\n",
      "97:\tlearn: 0.6954264\ttotal: 55.9s\tremaining: 1.14s\n",
      "98:\tlearn: 0.6955881\ttotal: 56.5s\tremaining: 571ms\n",
      "99:\tlearn: 0.6956285\ttotal: 57s\tremaining: 0us\n",
      "Accuracy: 69.64 %\n",
      "Time: 57.87 seconds\n"
     ]
    }
   ],
   "source": [
    "# !pip install catboost\n",
    "import catboost as cb\n",
    "start = time.time() # 시작 시간 지정\n",
    "cb_dtrain = cb.Pool(data = train_x, label = train_y) # 학습 데이터를 Catboost 모델에 맞게 변환\n",
    "cb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "            'eval_metric': 'Accuracy', # 평가 척도\n",
    "            'loss_function': 'MultiClass'} # 손실 함수, 목적 함수\n",
    "cb_model = cb.train(pool = cb_dtrain, params = cb_param) # 학습 진행\n",
    "cb_model_predict = np.argmax(cb_model.predict(test_x), axis = 1) + 1 # 평가 데이터 예측, Softmax의 결과값 중 가장 큰 값의 Label로 예측, 인덱스의 순서를 맞추기 위해 +1\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, cb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.35426047,  1.22109587,  0.44230101, ..., -0.1698448 ,\n",
       "        -0.02059177, -0.2130643 ],\n",
       "       [-0.07235138,  0.42535181,  0.20060428, ...,  0.21863604,\n",
       "         0.2719157 ,  0.25089315],\n",
       "       [-0.3315885 , -0.31862353, -0.31279765, ..., -0.29798357,\n",
       "        -0.24018767, -0.32984969],\n",
       "       ...,\n",
       "       [ 0.05304325,  0.02500267, -0.14752573, ..., -0.20741963,\n",
       "         0.12789417,  1.51166757],\n",
       "       [-0.55093666,  1.7691278 ,  0.99746884, ..., -0.3420542 ,\n",
       "        -0.49799871, -0.38136323],\n",
       "       [-0.3033724 ,  0.09352675, -0.11808658, ...,  0.65825036,\n",
       "         1.05515787, -0.20799899]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  floors  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00     1.0   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25     2.0   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00     1.0   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00     1.0   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00     1.0   \n",
       "\n",
       "   waterfront  condition  grade  yr_built  yr_renovated  zipcode      lat  \\\n",
       "0           0          3      7      1955             0    98178  47.5112   \n",
       "1           0          3      7      1951          1991    98125  47.7210   \n",
       "2           0          3      6      1933             0    98028  47.7379   \n",
       "3           0          5      7      1965             0    98136  47.5208   \n",
       "4           0          3      8      1987             0    98074  47.6168   \n",
       "\n",
       "      long  \n",
       "0 -122.257  \n",
       "1 -122.319  \n",
       "2 -122.233  \n",
       "3 -122.393  \n",
       "4 -122.045  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "data = pd.read_csv(\"../Data/kc_house_data.csv\") \n",
    "data.head() # 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['id', 'date', 'zipcode', 'lat', 'long'], axis = 1) # id, date, zipcode, lat, long  제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15129, 8) (6484, 8) (15129,) (6484,)\n"
     ]
    }
   ],
   "source": [
    "feature_columns = list(data.columns.difference(['price'])) # Price를 제외한 모든 행\n",
    "X = data[feature_columns]\n",
    "y = data['price']\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.3, random_state = 42) # 학습데이터와 평가데이터의 비율을 7:3\n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape) # 데이터 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 237\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537729.263666\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "# !pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "start = time.time() # 시작 시간 지정\n",
    "lgb_dtrain = lgb.Dataset(data = train_x, label = train_y) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "lgb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 500, # Number of trees, 트리 생성 개수\n",
    "            'objective': 'regression'} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([501716.63620816, 632581.56353778, 947111.30341027, ...,\n",
       "       341921.48670391, 923907.86981542, 457235.91311423])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210904.17249451784"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "sqrt(mean_squared_error(lgb_model.predict(test_x),test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9575\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000411 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 235\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 538529.222354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9573\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000200 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 230\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 535633.589332\n",
      "9583\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000226 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 234\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 538902.103312\n",
      "9592\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000198 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537036.260890\n",
      "9536\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000194 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 540305.584705\n",
      "9630\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000288 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 235\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 539485.527067\n",
      "9539\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000196 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 539400.753189\n",
      "9589\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000200 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 234\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 534395.810827\n",
      "9526\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000192 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537519.544121\n",
      "9524\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537268.513319\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "bagging_predict_result = [] # 빈 리스트 생성\n",
    "for _ in range(10):\n",
    "    data_index = [data_index for data_index in range(train_x.shape[0])] # 학습 데이터의 인덱스를 리스트로 변환\n",
    "    random_data_index = np.random.choice(data_index, train_x.shape[0]) # 데이터의 1/10 크기만큼 랜덤 샘플링, // 는 소수점을 무시하기 위함\n",
    "    print(len(set(random_data_index)))\n",
    "    lgb_dtrain = lgb.Dataset(data = train_x.iloc[random_data_index,], label = train_y.iloc[random_data_index,]) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "    lgb_param = {'max_depth': 14, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 500, # Number of trees, 트리 생성 개수\n",
    "            'objective': 'regression'} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "    lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n",
    "    predict1 = lgb_model.predict(test_x) # 테스트 데이터 예측\n",
    "    bagging_predict_result.append(predict1) # 반복문이 실행되기 전 빈 리스트에 결과 값 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([478955.90257183, 640970.7626976 , 976347.84846491, ...,\n",
       "        349777.73255951, 883900.62750953, 469302.53661832]),\n",
       " array([501050.9846588 , 647425.96178879, 947532.77395537, ...,\n",
       "        322360.91420847, 895068.21536452, 454029.13794898]),\n",
       " array([511001.53959   , 599632.9339814 , 957204.42456489, ...,\n",
       "        329521.91148922, 855463.00271202, 460219.05443754]),\n",
       " array([ 508666.2688509 ,  596010.96364577, 1005896.0873818 , ...,\n",
       "         336515.42961   ,  939542.85325271,  444920.14705449]),\n",
       " array([ 490723.44902634,  632985.76566379,  915263.64338255, ...,\n",
       "         330400.5589343 , 1021747.95099327,  460378.70965189]),\n",
       " array([505211.71347085, 672593.14575019, 896820.37786699, ...,\n",
       "        339357.84714135, 907084.97124631, 492203.79522671]),\n",
       " array([493203.03562025, 656998.96748117, 967030.88055256, ...,\n",
       "        328370.58272108, 753713.92721064, 470368.42294979]),\n",
       " array([493811.93320632, 619509.18979029, 953867.10379781, ...,\n",
       "        327564.18610259, 848688.05385635, 474286.53961635]),\n",
       " array([519838.245214  , 665076.42494421, 843794.90311915, ...,\n",
       "        334384.80108251, 933275.33349675, 456786.91189821]),\n",
       " array([487168.7332581 , 647581.67701923, 920637.94777127, ...,\n",
       "        344322.32163562, 848046.18722006, 450912.26922842])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging을 바탕으로 예측한 결과값에 대한 평균을 계산\n",
    "bagging_predict = [] # 빈 리스트 생성\n",
    "for lst2_index in range(test_x.shape[0]): # 테스트 데이터 개수만큼의 반복\n",
    "    temp_predict = [] # 임시 빈 리스트 생성 (반복문 내 결과값 저장)\n",
    "    for lst_index in range(len(bagging_predict_result)): # Bagging 결과 리스트 반복\n",
    "        temp_predict.append(bagging_predict_result[lst_index][lst2_index]) # 각 Bagging 결과 예측한 값 중 같은 인덱스를 리스트에 저장\n",
    "    bagging_predict.append(np.mean(temp_predict)) # 해당 인덱스의 30개의 결과값에 대한 평균을 최종 리스트에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 210851.9299653197\n"
     ]
    }
   ],
   "source": [
    "# 예측한 결과값들의 평균을 계산하여 실제 테스트 데이트의 타겟변수와 비교하여 성능 평가\n",
    "\n",
    "print(\"RMSE: {}\".format(sqrt(mean_squared_error(bagging_predict, test_y)))) # RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[498963.1805467385,\n",
       " 637878.579276245,\n",
       " 938439.5990857298,\n",
       " 1634132.704308533,\n",
       " 634281.2174750556,\n",
       " 370477.7521384684,\n",
       " 707961.2684062438,\n",
       " 436003.0382098917,\n",
       " 464835.4113638092,\n",
       " 498142.079590635,\n",
       " 634140.3734165436,\n",
       " 370045.8071382266,\n",
       " 298482.6023140402,\n",
       " 358540.0837627789,\n",
       " 342922.4119673932,\n",
       " 1326353.157322278,\n",
       " 362181.8539360726,\n",
       " 1008193.3158232694,\n",
       " 314273.9328836982,\n",
       " 523460.6528589573,\n",
       " 379724.5682515555,\n",
       " 1929513.0209870867,\n",
       " 657872.4812112093,\n",
       " 565478.0080644779,\n",
       " 500589.98963082826,\n",
       " 486112.1111164594,\n",
       " 296005.96143626573,\n",
       " 252615.49957250315,\n",
       " 476806.3201624289,\n",
       " 536530.5374618417,\n",
       " 489737.3431937912,\n",
       " 471623.40783913934,\n",
       " 462958.58514241566,\n",
       " 548796.0931240828,\n",
       " 376952.9309677484,\n",
       " 1054281.8162695162,\n",
       " 943813.0793265136,\n",
       " 535659.9441485533,\n",
       " 359189.49533903354,\n",
       " 1457791.8551973812,\n",
       " 396336.1963211778,\n",
       " 273003.39787304023,\n",
       " 510493.0991501155,\n",
       " 343537.9389220658,\n",
       " 255309.50555370483,\n",
       " 246070.21859089722,\n",
       " 330987.9860462382,\n",
       " 334621.1113691288,\n",
       " 355051.4164202789,\n",
       " 572364.0351796227,\n",
       " 371121.1812165006,\n",
       " 344872.3815526029,\n",
       " 765217.6442770037,\n",
       " 337672.03610543074,\n",
       " 465534.11687448167,\n",
       " 1711136.1004947177,\n",
       " 480452.6932989762,\n",
       " 708433.6960623993,\n",
       " 330361.3861370559,\n",
       " 651639.2727972625,\n",
       " 482135.60388870805,\n",
       " 373795.18436189543,\n",
       " 298455.2502378778,\n",
       " 514097.9078609346,\n",
       " 443549.08031678165,\n",
       " 282373.558421454,\n",
       " 380358.2402486338,\n",
       " 1635406.3483452182,\n",
       " 484491.19909817417,\n",
       " 662413.2447372683,\n",
       " 430636.3380064074,\n",
       " 298383.6103489882,\n",
       " 763544.7949988468,\n",
       " 528118.4406988069,\n",
       " 511827.1080025025,\n",
       " 1274553.4948141314,\n",
       " 815014.9806604326,\n",
       " 288357.67225039715,\n",
       " 457019.15540277417,\n",
       " 912919.165836343,\n",
       " 634022.1591511951,\n",
       " 378217.08871267055,\n",
       " 650110.9536179114,\n",
       " 361123.8863771543,\n",
       " 820783.6216190525,\n",
       " 527684.240224015,\n",
       " 529036.9396730714,\n",
       " 556117.6832361845,\n",
       " 357702.0082846929,\n",
       " 455683.68172167876,\n",
       " 352555.26255922194,\n",
       " 394856.9747015205,\n",
       " 645699.5747870668,\n",
       " 1066548.3711003987,\n",
       " 429272.9442295217,\n",
       " 497544.2643300601,\n",
       " 359956.21594553284,\n",
       " 307291.04932258173,\n",
       " 836245.350589886,\n",
       " 455190.7667515479,\n",
       " 257509.84252980546,\n",
       " 905256.7768383814,\n",
       " 980913.0936103382,\n",
       " 486106.96487264446,\n",
       " 1014852.3968438238,\n",
       " 297960.8339624268,\n",
       " 497523.8966364581,\n",
       " 491354.8000311215,\n",
       " 798370.1567770618,\n",
       " 2430940.4300853154,\n",
       " 553564.2347905661,\n",
       " 332804.05432299234,\n",
       " 563964.8163267274,\n",
       " 621643.3839396221,\n",
       " 554617.5378321818,\n",
       " 336663.115556541,\n",
       " 310407.144747388,\n",
       " 249038.64962267262,\n",
       " 320256.95993527886,\n",
       " 342922.4119673932,\n",
       " 379831.0964572128,\n",
       " 285559.7377899045,\n",
       " 334030.7730781208,\n",
       " 257530.85591306275,\n",
       " 597116.5105225022,\n",
       " 648314.4368651977,\n",
       " 277913.4591955001,\n",
       " 765417.7102434261,\n",
       " 458826.15523658943,\n",
       " 425744.00553055544,\n",
       " 532683.1061634425,\n",
       " 473750.38967844576,\n",
       " 410035.0534550435,\n",
       " 850607.2369364069,\n",
       " 376472.150782352,\n",
       " 458470.71454575856,\n",
       " 381601.6982944504,\n",
       " 354007.4954631533,\n",
       " 906652.5410256272,\n",
       " 616230.5009820992,\n",
       " 510727.1670488375,\n",
       " 787312.4585190237,\n",
       " 908530.7710086675,\n",
       " 404098.32173256786,\n",
       " 258482.8868330989,\n",
       " 384409.84057108435,\n",
       " 479798.5443451806,\n",
       " 247651.68998611282,\n",
       " 413672.1483523244,\n",
       " 477837.73672049225,\n",
       " 573947.4707047299,\n",
       " 713532.1147764884,\n",
       " 558623.000628683,\n",
       " 1113994.2530089286,\n",
       " 949897.505999151,\n",
       " 831070.2876425702,\n",
       " 591285.0873217301,\n",
       " 651190.3652631929,\n",
       " 587327.2433394382,\n",
       " 491860.3119162638,\n",
       " 637940.712722399,\n",
       " 368606.96094983735,\n",
       " 334621.1113691288,\n",
       " 357003.26103036845,\n",
       " 360013.6986143209,\n",
       " 341207.1166191776,\n",
       " 300742.7295697592,\n",
       " 310608.0756710534,\n",
       " 452144.24667056726,\n",
       " 462491.4992394564,\n",
       " 621414.0390404174,\n",
       " 393216.8300821684,\n",
       " 467526.99206154264,\n",
       " 573247.8125897757,\n",
       " 424714.54336824163,\n",
       " 417992.0505648861,\n",
       " 363093.212908812,\n",
       " 660986.1018595314,\n",
       " 346521.54620041675,\n",
       " 257175.56716362503,\n",
       " 308839.65785016515,\n",
       " 482100.79254209576,\n",
       " 538647.3244590277,\n",
       " 667231.5486936474,\n",
       " 465172.1462599997,\n",
       " 469342.10759563,\n",
       " 276140.9681413262,\n",
       " 435949.0225424032,\n",
       " 352503.3039282903,\n",
       " 350124.41900047514,\n",
       " 371093.12061720545,\n",
       " 652357.8268647841,\n",
       " 1626252.3702915125,\n",
       " 1248533.3485803339,\n",
       " 266011.08481826825,\n",
       " 487768.6162306516,\n",
       " 499796.545916248,\n",
       " 1588323.6613439072,\n",
       " 469581.24442704243,\n",
       " 465221.0046275719,\n",
       " 324705.2932751637,\n",
       " 381009.27963212423,\n",
       " 517690.55799351557,\n",
       " 777684.8199423498,\n",
       " 786537.3914891839,\n",
       " 311689.0283268454,\n",
       " 500589.98963082826,\n",
       " 306686.3465216488,\n",
       " 506777.7511808862,\n",
       " 1390017.583413512,\n",
       " 359956.21594553284,\n",
       " 419550.493263583,\n",
       " 456017.60334915324,\n",
       " 359956.21594553284,\n",
       " 328375.24502230575,\n",
       " 711557.6211894131,\n",
       " 788970.2680160516,\n",
       " 347318.8860495056,\n",
       " 367908.47275870666,\n",
       " 361765.90465859085,\n",
       " 1696925.8057585421,\n",
       " 544008.9374967113,\n",
       " 504204.2674333501,\n",
       " 453801.5938289471,\n",
       " 532569.7947818309,\n",
       " 749026.5359905653,\n",
       " 346367.5568094795,\n",
       " 1206575.5081258419,\n",
       " 899259.6131253701,\n",
       " 464835.4113638092,\n",
       " 356737.5711357121,\n",
       " 481202.7075889567,\n",
       " 716392.6817703252,\n",
       " 298088.27518284705,\n",
       " 340751.1189380769,\n",
       " 386919.8928385816,\n",
       " 352305.2503927382,\n",
       " 354981.0419549129,\n",
       " 2520786.8220308814,\n",
       " 354503.177627016,\n",
       " 440799.05509449507,\n",
       " 464007.8509779618,\n",
       " 607284.9232251048,\n",
       " 415259.4414461905,\n",
       " 471651.6667479823,\n",
       " 307571.0500126921,\n",
       " 522961.723786097,\n",
       " 562345.1391631981,\n",
       " 714996.8192623706,\n",
       " 868122.8480585988,\n",
       " 530636.5414813554,\n",
       " 432449.60664499213,\n",
       " 712592.9416272931,\n",
       " 358851.9510094475,\n",
       " 350124.41900047514,\n",
       " 522858.069031867,\n",
       " 496941.9908722897,\n",
       " 471803.6161211558,\n",
       " 892809.7361023675,\n",
       " 369632.77432264976,\n",
       " 3456678.530578236,\n",
       " 632529.9737868715,\n",
       " 760555.9381539281,\n",
       " 1084721.691351195,\n",
       " 501954.23673383036,\n",
       " 663440.5194846184,\n",
       " 899727.4345635436,\n",
       " 350196.89384221245,\n",
       " 716470.1975709691,\n",
       " 437808.7443861289,\n",
       " 472885.2899432291,\n",
       " 342637.7669394173,\n",
       " 285533.77897607407,\n",
       " 436168.9909022267,\n",
       " 412243.26773006754,\n",
       " 1439890.922616824,\n",
       " 302206.1073601878,\n",
       " 346604.16016330884,\n",
       " 539870.2716351971,\n",
       " 363552.37005329237,\n",
       " 311083.93630734517,\n",
       " 509667.1643369306,\n",
       " 367341.8797606254,\n",
       " 444518.8949622441,\n",
       " 486847.67363668804,\n",
       " 448279.98030880594,\n",
       " 370120.1997788093,\n",
       " 632502.2768817149,\n",
       " 355362.12085896183,\n",
       " 317254.31264097465,\n",
       " 800077.1786788262,\n",
       " 446899.55144420837,\n",
       " 258569.69256913924,\n",
       " 339105.313508042,\n",
       " 652357.8268647841,\n",
       " 653434.3528048357,\n",
       " 494059.2918812409,\n",
       " 443320.9619761288,\n",
       " 449385.40093589557,\n",
       " 582903.4053426841,\n",
       " 473271.6441049433,\n",
       " 545724.838245598,\n",
       " 330863.24218990334,\n",
       " 570361.7963743758,\n",
       " 345175.6882146507,\n",
       " 864846.2307331723,\n",
       " 452144.24667056726,\n",
       " 390141.1705182901,\n",
       " 334981.93105479993,\n",
       " 327724.40122748417,\n",
       " 367045.71569081873,\n",
       " 290657.0619301206,\n",
       " 838275.067443135,\n",
       " 1646225.4845624338,\n",
       " 967617.8336177736,\n",
       " 451272.0660152206,\n",
       " 827007.7899853546,\n",
       " 467691.5326683352,\n",
       " 813481.7123777971,\n",
       " 346782.2124058626,\n",
       " 396871.94840147113,\n",
       " 499308.6899865212,\n",
       " 266011.08481826825,\n",
       " 298471.1023232372,\n",
       " 460939.9624504082,\n",
       " 462491.4992394564,\n",
       " 582548.7694061395,\n",
       " 298108.2683036997,\n",
       " 514701.3239304046,\n",
       " 257530.85591306275,\n",
       " 657668.678802808,\n",
       " 288360.20284937817,\n",
       " 494754.7218880154,\n",
       " 297122.00674909126,\n",
       " 388581.13541325805,\n",
       " 483547.60170678503,\n",
       " 624401.7235796074,\n",
       " 474215.68426424015,\n",
       " 932318.9561028291,\n",
       " 259775.51797753148,\n",
       " 1694150.440370749,\n",
       " 474046.7737890605,\n",
       " 440257.20124563295,\n",
       " 571097.2002033748,\n",
       " 677651.050665594,\n",
       " 616876.857239109,\n",
       " 343126.52437006024,\n",
       " 462463.97808553866,\n",
       " 465700.7614190012,\n",
       " 725133.281413698,\n",
       " 285380.5201177895,\n",
       " 363748.494088405,\n",
       " 477632.9083773404,\n",
       " 603916.6712344978,\n",
       " 340210.0912935477,\n",
       " 860968.7280649852,\n",
       " 562039.9473444612,\n",
       " 925834.4497215997,\n",
       " 947204.0890425548,\n",
       " 634615.6940730468,\n",
       " 369964.83609256597,\n",
       " 796428.0433420939,\n",
       " 358674.2331059511,\n",
       " 569181.6421696333,\n",
       " 669117.545418572,\n",
       " 330848.7600711504,\n",
       " 760282.5698611583,\n",
       " 403005.37445168727,\n",
       " 1005354.3509718925,\n",
       " 383699.4525273405,\n",
       " 508003.64749163,\n",
       " 650814.2310463018,\n",
       " 573523.0629017506,\n",
       " 352244.8607187882,\n",
       " 548383.0551888696,\n",
       " 376545.2473169875,\n",
       " 339437.6548764672,\n",
       " 549435.2702423604,\n",
       " 387359.7552411727,\n",
       " 418437.34530888346,\n",
       " 373570.81093736994,\n",
       " 709105.3181205413,\n",
       " 649434.6754649204,\n",
       " 438416.9867402458,\n",
       " 463271.8951989971,\n",
       " 458346.856309691,\n",
       " 299237.9198123803,\n",
       " 492446.32952784793,\n",
       " 422108.68415189645,\n",
       " 465021.90707530116,\n",
       " 575665.1529685825,\n",
       " 384211.79308871354,\n",
       " 376941.2816516635,\n",
       " 405246.94168973423,\n",
       " 1462055.4052122007,\n",
       " 383699.4525273405,\n",
       " 346958.49813737225,\n",
       " 1197378.1453859743,\n",
       " 652197.0284471263,\n",
       " 384008.82361598953,\n",
       " 430636.3380064074,\n",
       " 482327.46151853737,\n",
       " 651520.2239836125,\n",
       " 455701.90974997095,\n",
       " 394120.5082359253,\n",
       " 437113.3880799756,\n",
       " 462491.4992394564,\n",
       " 425744.00553055544,\n",
       " 485053.7641555342,\n",
       " 469238.7893985322,\n",
       " 348367.4549144753,\n",
       " 474084.9874705345,\n",
       " 616166.7053051355,\n",
       " 423303.0445176357,\n",
       " 268448.3812863183,\n",
       " 379088.3639607423,\n",
       " 464737.70561014663,\n",
       " 451662.49875628576,\n",
       " 1111632.8494260092,\n",
       " 455541.27670159575,\n",
       " 400190.6228741418,\n",
       " 565295.4165714205,\n",
       " 334182.2541799642,\n",
       " 672962.7081713048,\n",
       " 404842.4113886283,\n",
       " 474589.43460246467,\n",
       " 485360.0051619491,\n",
       " 468078.89061339817,\n",
       " 564546.541993299,\n",
       " 334541.2088055169,\n",
       " 614576.4909911003,\n",
       " 523165.61757810565,\n",
       " 701011.3710088066,\n",
       " 538463.6453732278,\n",
       " 520391.2212061274,\n",
       " 491305.76788040716,\n",
       " 526548.4273061234,\n",
       " 386905.2154765009,\n",
       " 364749.5245238848,\n",
       " 354260.85713956115,\n",
       " 641646.1488315678,\n",
       " 379435.36370144004,\n",
       " 356856.8118120658,\n",
       " 274084.1851878207,\n",
       " 373962.0353634682,\n",
       " 380832.6180425842,\n",
       " 850607.2369364069,\n",
       " 2273423.754277039,\n",
       " 445596.61619013443,\n",
       " 1179743.5481613176,\n",
       " 509688.5532944404,\n",
       " 267374.29445789324,\n",
       " 485224.5480658822,\n",
       " 452453.7725010299,\n",
       " 428370.18446435797,\n",
       " 676019.8968032962,\n",
       " 381495.63827311614,\n",
       " 829579.2329728361,\n",
       " 373685.9503384804,\n",
       " 276581.47175208665,\n",
       " 459340.5493488808,\n",
       " 633969.0187341368,\n",
       " 954961.135295356,\n",
       " 375963.77524124726,\n",
       " 715102.0362331059,\n",
       " 452881.2685966152,\n",
       " 356863.3204970354,\n",
       " 544070.6045096844,\n",
       " 978365.9799102657,\n",
       " 433117.3661938154,\n",
       " 474215.68426424015,\n",
       " 348367.4549144753,\n",
       " 455943.78150261444,\n",
       " 1175655.6384627668,\n",
       " 447776.1216986591,\n",
       " 497862.83601782034,\n",
       " 392937.00158175087,\n",
       " 451699.1520217949,\n",
       " 313867.3472017726,\n",
       " 454402.991532271,\n",
       " 300335.94256662123,\n",
       " 336488.9721235392,\n",
       " 345192.58483188506,\n",
       " 386774.771203359,\n",
       " 440478.3694924839,\n",
       " 615017.1539609435,\n",
       " 523983.466574414,\n",
       " 335672.7479034657,\n",
       " 748276.5078380015,\n",
       " 665399.4336825684,\n",
       " 702580.9491961394,\n",
       " 341368.97825214744,\n",
       " 424714.54336824163,\n",
       " 654462.1788681885,\n",
       " 1798071.69275708,\n",
       " 499560.81279899983,\n",
       " 682718.6957019226,\n",
       " 368667.28540270345,\n",
       " 368692.11178771657,\n",
       " 345783.121190898,\n",
       " 720423.0921369721,\n",
       " 482135.60388870805,\n",
       " 317460.28008213185,\n",
       " 525537.850629322,\n",
       " 348763.1653990662,\n",
       " 568629.1187019094,\n",
       " 786577.1260029876,\n",
       " 1012475.6871313952,\n",
       " 324856.2500821495,\n",
       " 535161.6018097817,\n",
       " 1212742.7287249726,\n",
       " 462491.4992394564,\n",
       " 806529.1369006016,\n",
       " 621601.3166663436,\n",
       " 459677.91053121694,\n",
       " 820820.2252545714,\n",
       " 715635.5411661206,\n",
       " 255213.2729244957,\n",
       " 373795.18436189543,\n",
       " 284217.21351284155,\n",
       " 432776.40234934987,\n",
       " 543966.0966266523,\n",
       " 334325.2067203211,\n",
       " 493286.4376495948,\n",
       " 529849.3781258246,\n",
       " 415579.6790796626,\n",
       " 495860.0125509198,\n",
       " 357300.8072961179,\n",
       " 832416.4113959109,\n",
       " 343652.12181181804,\n",
       " 375001.94392223726,\n",
       " 377447.73108039814,\n",
       " 1183887.2469878981,\n",
       " 463271.8951989971,\n",
       " 297613.7790761294,\n",
       " 282254.90998823347,\n",
       " 467384.9136586039,\n",
       " 821807.7053363491,\n",
       " 474219.4830958899,\n",
       " 671962.7920962455,\n",
       " 550870.4388127967,\n",
       " 453403.54449106165,\n",
       " 978321.032636477,\n",
       " 282189.1158315118,\n",
       " 803746.9237780509,\n",
       " 632744.3675350395,\n",
       " 527757.6359690975,\n",
       " 1019267.113437353,\n",
       " 462065.06321927626,\n",
       " 287658.97295583563,\n",
       " 828511.9829795145,\n",
       " 345377.0231130484,\n",
       " 627960.3345514553,\n",
       " 1476657.6028090892,\n",
       " 348285.09698185365,\n",
       " 509326.95174511,\n",
       " 452288.3091729803,\n",
       " 369825.38933650637,\n",
       " 255218.4069261794,\n",
       " 861585.0035344486,\n",
       " 435900.28919735504,\n",
       " 297403.47482223785,\n",
       " 466794.7238004633,\n",
       " 355536.240697276,\n",
       " 297499.5803226883,\n",
       " 500804.7138941015,\n",
       " 359470.7381624703,\n",
       " 412798.2954507947,\n",
       " 390351.9050763982,\n",
       " 474809.62142001104,\n",
       " 485437.4090753432,\n",
       " 547210.862764235,\n",
       " 784494.9915687914,\n",
       " 335047.0184465577,\n",
       " 330191.22505115636,\n",
       " 379435.36370144004,\n",
       " 381009.27963212423,\n",
       " 330108.05140895594,\n",
       " 385058.55381834053,\n",
       " 776394.4293408244,\n",
       " 501273.8335650295,\n",
       " 829136.8131018211,\n",
       " 852129.2706989773,\n",
       " 466989.3460263553,\n",
       " 439440.4252021952,\n",
       " 483625.2083384808,\n",
       " 312543.6184516572,\n",
       " 404016.8199185605,\n",
       " 440017.6191400982,\n",
       " 305649.47930981667,\n",
       " 475554.02788427577,\n",
       " 671962.7920962455,\n",
       " 429699.7907402739,\n",
       " 409341.55316812627,\n",
       " 465582.088260967,\n",
       " 415241.3669747014,\n",
       " 1026126.3908263125,\n",
       " 721444.2150030282,\n",
       " 759497.1160027356,\n",
       " 366862.4096886649,\n",
       " 458346.856309691,\n",
       " 399022.09138982935,\n",
       " 563955.700962379,\n",
       " 259986.92608313193,\n",
       " 385932.60116331826,\n",
       " 360588.4526316963,\n",
       " 312903.8257794411,\n",
       " 356954.8826646327,\n",
       " 283999.4757774746,\n",
       " 411789.98508435395,\n",
       " 588944.9496358742,\n",
       " 309019.42390236957,\n",
       " 442983.49471974873,\n",
       " 518141.1091454018,\n",
       " 493464.6208043505,\n",
       " 350044.654912578,\n",
       " 337466.3049561867,\n",
       " 282727.76623199484,\n",
       " 534599.3762950024,\n",
       " 473091.7118277207,\n",
       " 1733484.9868419133,\n",
       " 459677.91053121694,\n",
       " 2187995.6110535585,\n",
       " 688978.1102665489,\n",
       " 1009398.0167987002,\n",
       " 608329.2733779805,\n",
       " 415614.0614693978,\n",
       " 453193.31584898214,\n",
       " 929661.823374413,\n",
       " 699482.5946463695,\n",
       " 482184.73978363426,\n",
       " 471098.73050414224,\n",
       " 755428.4666158285,\n",
       " 500008.4610286738,\n",
       " 383424.0061166204,\n",
       " 398541.2124612493,\n",
       " 364293.8172694581,\n",
       " 469497.47128187335,\n",
       " 331615.65353657445,\n",
       " 731718.5063518671,\n",
       " 503985.4982662699,\n",
       " 796428.0433420939,\n",
       " 694215.8252101879,\n",
       " 508910.99904780276,\n",
       " 449658.0853947988,\n",
       " 491235.31229620957,\n",
       " 490809.4908062011,\n",
       " 504999.54564397584,\n",
       " 553440.8996947662,\n",
       " 307535.6502952358,\n",
       " 1071021.637837936,\n",
       " 307900.5909466534,\n",
       " 588921.1556819228,\n",
       " 277900.1014240167,\n",
       " 1922314.4861098186,\n",
       " 822992.2921168625,\n",
       " 650202.081423078,\n",
       " 521715.6333366569,\n",
       " 662428.3110234516,\n",
       " 560091.4430380657,\n",
       " 348763.1653990662,\n",
       " 532860.0532020024,\n",
       " 468425.53840797785,\n",
       " 986484.3876759307,\n",
       " 375221.1558300046,\n",
       " 504569.15347857634,\n",
       " 965939.4829314377,\n",
       " 532705.6696954193,\n",
       " 461293.9142066677,\n",
       " 452589.6023131488,\n",
       " 1037125.5669860229,\n",
       " 316052.39480971533,\n",
       " 630218.9002675216,\n",
       " 787965.2806209067,\n",
       " 316052.39480971533,\n",
       " 621741.6003220818,\n",
       " 614000.2668005334,\n",
       " 400229.6281511253,\n",
       " 477954.8870866593,\n",
       " 620912.2297813717,\n",
       " 431190.90201919636,\n",
       " 341368.97825214744,\n",
       " 534662.5378400385,\n",
       " 277900.1014240167,\n",
       " 352257.43067487376,\n",
       " 469632.8110687335,\n",
       " 537464.8662109704,\n",
       " 438424.2295250034,\n",
       " 802376.4796018967,\n",
       " 412267.81518485246,\n",
       " 298482.6023140402,\n",
       " 324939.6603450588,\n",
       " 469238.7893985322,\n",
       " 754806.6819475957,\n",
       " 780237.5775419296,\n",
       " 369632.77432264976,\n",
       " 319354.12067880295,\n",
       " 386121.86127421923,\n",
       " 272625.3547285496,\n",
       " 843540.2912463214,\n",
       " 398271.91439644306,\n",
       " 285003.7712941719,\n",
       " 341785.6347015301,\n",
       " 491909.6891401248,\n",
       " 494952.5208404365,\n",
       " 469238.7893985322,\n",
       " 1132672.1409981304,\n",
       " 1023791.6354655722,\n",
       " 1224139.4657046995,\n",
       " 435628.05603358755,\n",
       " 598145.5044075747,\n",
       " 364913.0421197462,\n",
       " 522849.12065386417,\n",
       " 357589.7280337107,\n",
       " 391770.26755443774,\n",
       " 348253.39889867755,\n",
       " 834088.6085035524,\n",
       " 465534.11687448167,\n",
       " 353757.6618786546,\n",
       " 393385.4488442665,\n",
       " 287680.1929225129,\n",
       " 533313.8142541265,\n",
       " 484087.49850959313,\n",
       " 425837.08343757957,\n",
       " 530356.1357737908,\n",
       " 283890.33788424014,\n",
       " 632494.575854306,\n",
       " 525412.1965587272,\n",
       " 457019.15540277417,\n",
       " 473271.6441049433,\n",
       " 298286.44582563115,\n",
       " 360288.27771544905,\n",
       " 1097272.6874947795,\n",
       " 779868.1515828483,\n",
       " 439794.6587294096,\n",
       " 316202.9799625239,\n",
       " 352973.2222343714,\n",
       " 1037048.8358648485,\n",
       " 552187.38867194,\n",
       " 425349.4605231077,\n",
       " 277479.09579092014,\n",
       " 381685.3566930786,\n",
       " 294940.10183909745,\n",
       " 831391.3438875766,\n",
       " 255194.11807006114,\n",
       " 325497.2872275321,\n",
       " 296005.96143626573,\n",
       " 471651.6667479823,\n",
       " 481651.1538947313,\n",
       " 603428.5340523643,\n",
       " 587175.9463194895,\n",
       " 386173.4831697408,\n",
       " 481383.5025862934,\n",
       " 566178.2507779185,\n",
       " 484771.32965117553,\n",
       " 471692.1632186302,\n",
       " 616230.5009820992,\n",
       " 513344.1937663666,\n",
       " 798127.2011340108,\n",
       " 385099.9420121344,\n",
       " 303205.93776811747,\n",
       " 470008.6592538262,\n",
       " 580978.6961852934,\n",
       " 478350.45487977704,\n",
       " 489473.0487824575,\n",
       " 912184.2678500621,\n",
       " 531875.781242416,\n",
       " 736342.369860534,\n",
       " 620238.2951372748,\n",
       " 799420.1427793355,\n",
       " 277456.3865007049,\n",
       " 468936.69196662697,\n",
       " 468496.909407303,\n",
       " 840858.0544812398,\n",
       " 916399.8774048041,\n",
       " 283234.10900779755,\n",
       " 258482.8868330989,\n",
       " 491796.368438949,\n",
       " 289418.8836916159,\n",
       " 726179.7459474222,\n",
       " 390138.00912088546,\n",
       " 475359.0326693555,\n",
       " 518427.7218873201,\n",
       " 387482.3048909636,\n",
       " 296005.96143626573,\n",
       " 291145.3285211464,\n",
       " 331541.48671586317,\n",
       " 636255.9938316435,\n",
       " 284464.0850698714,\n",
       " 372948.2005600593,\n",
       " 476304.5513170462,\n",
       " 388099.28801617434,\n",
       " 526548.4273061234,\n",
       " 369980.42211847205,\n",
       " 391770.26755443774,\n",
       " 465534.11687448167,\n",
       " 348997.4323111471,\n",
       " 1212387.4277612483,\n",
       " 616151.6467187256,\n",
       " 331887.80023023224,\n",
       " 461217.72322058363,\n",
       " 768597.9229695259,\n",
       " 627503.9336908648,\n",
       " 276688.2793125148,\n",
       " 470524.2981413451,\n",
       " 388581.13541325805,\n",
       " 298099.89843747654,\n",
       " 869455.6824374611,\n",
       " 466541.46368440584,\n",
       " 554047.1973377095,\n",
       " 497203.3441448415,\n",
       " 669779.9343362183,\n",
       " 578834.9819648899,\n",
       " 892745.8827911395,\n",
       " 662413.2447372683,\n",
       " 779893.0091193437,\n",
       " 375109.20325259835,\n",
       " 1014853.8717327311,\n",
       " 656021.0442202629,\n",
       " 774594.8328030444,\n",
       " 468263.5454397329,\n",
       " 471368.15768046025,\n",
       " 381495.63827311614,\n",
       " 465534.11687448167,\n",
       " 373795.18436189543,\n",
       " 787312.4585190237,\n",
       " 515670.99719894805,\n",
       " 492145.215893066,\n",
       " 278980.6714553534,\n",
       " 277900.1014240167,\n",
       " 298234.63426598604,\n",
       " 800204.0189854706,\n",
       " 651639.2727972625,\n",
       " 458123.2792906187,\n",
       " 361317.2789236131,\n",
       " 456600.5612057463,\n",
       " 367045.71569081873,\n",
       " 651080.2158628191,\n",
       " 277915.3056898221,\n",
       " 645910.3577573021,\n",
       " 527546.2983644133,\n",
       " 351655.68243206566,\n",
       " 276076.2370707677,\n",
       " 484491.19909817417,\n",
       " 505100.1442742781,\n",
       " 376787.52194469183,\n",
       " 688783.3943120788,\n",
       " 336488.9721235392,\n",
       " 390138.00912088546,\n",
       " 542013.1641798738,\n",
       " 300671.5951153018,\n",
       " 567154.7001949137,\n",
       " 684581.4071937177,\n",
       " 697301.5177777209,\n",
       " 492795.45155168016,\n",
       " 418509.78265385015,\n",
       " 711318.7587444042,\n",
       " 512820.13533577573,\n",
       " 651639.2727972625,\n",
       " 1559893.4084092288,\n",
       " 549921.4709456387,\n",
       " 340685.1110097252,\n",
       " 702449.7193517111,\n",
       " 827520.3140317423,\n",
       " 343051.3719559573,\n",
       " 404556.96937770053,\n",
       " 378316.3280714935,\n",
       " 292131.0859813743,\n",
       " 506015.5022733339,\n",
       " 505042.1477820322,\n",
       " 405366.89616641507,\n",
       " 496380.9554324245,\n",
       " 330776.95247719635,\n",
       " 371084.2516840676,\n",
       " 255265.1974241206,\n",
       " 354007.4954631533,\n",
       " 346958.49813737225,\n",
       " 331205.61278935353,\n",
       " 346448.0274960298,\n",
       " 333081.4143020249,\n",
       " 830765.0102988535,\n",
       " 416863.7562186925,\n",
       " 441246.1110224912,\n",
       " 709430.6909549006,\n",
       " 357426.59831669595,\n",
       " 255194.11807006114,\n",
       " 370819.12831844925,\n",
       " 291682.65199211106,\n",
       " 579648.4597669689,\n",
       " 301494.0330266265,\n",
       " 518253.3449448752,\n",
       " 813073.1165673401,\n",
       " 578475.3150842885,\n",
       " 561165.4454031693,\n",
       " 496529.1711399938,\n",
       " 267158.33349735884,\n",
       " 988249.3915596718,\n",
       " 634281.2174750556,\n",
       " 868122.8480585988,\n",
       " 323224.03954840207,\n",
       " 465794.6974364181,\n",
       " 797675.7655892794,\n",
       " 512574.16531118535,\n",
       " 257419.80717211962,\n",
       " 1128002.5685458486,\n",
       " 308828.03459553566,\n",
       " 630744.4571815787,\n",
       " 466794.7238004633,\n",
       " 794462.5473311234,\n",
       " 573391.8160036269,\n",
       " 452881.2685966152,\n",
       " 458346.856309691,\n",
       " 446525.9649989171,\n",
       " 553799.8140410874,\n",
       " 1307951.6939447732,\n",
       " 474676.8544529922,\n",
       " 840388.3650123819,\n",
       " 485717.60100557853,\n",
       " 380627.7631899221,\n",
       " 273009.07078197214,\n",
       " 433671.07184946595,\n",
       " 556487.7565040262,\n",
       " 496206.01433319866,\n",
       " 454152.62544278975,\n",
       " 294940.10183909745,\n",
       " 364342.3273437112,\n",
       " 350859.81175903056,\n",
       " 515002.5755577459,\n",
       " 568484.2936783093,\n",
       " 282480.89467496506,\n",
       " 381723.2045253497,\n",
       " 506455.33690991317,\n",
       " 621741.6003220818,\n",
       " 946508.7717617791,\n",
       " 662510.002753536,\n",
       " 358382.353188826,\n",
       " 789324.64012842,\n",
       " 718237.7478079824,\n",
       " 848179.9549099781,\n",
       " 354795.9773119515,\n",
       " 2196959.0022134427,\n",
       " 368511.74152613076,\n",
       " 382993.0599561115,\n",
       " 508961.4568226651,\n",
       " 333863.7030822711,\n",
       " 358212.3091653496,\n",
       " 548771.2163929408,\n",
       " 669831.4746700497,\n",
       " 736713.5399585261,\n",
       " 276666.0987153825,\n",
       " 352244.8607187882,\n",
       " 566177.8183872129,\n",
       " 2343773.5006368207,\n",
       " 553763.3440589576,\n",
       " 470057.16932807927,\n",
       " 905781.7290431559,\n",
       " 617568.2225258723,\n",
       " 549806.0152796612,\n",
       " 799569.2816152552,\n",
       " 711161.2898458855,\n",
       " 452881.2685966152,\n",
       " 298181.3963580377,\n",
       " 435083.17923697375,\n",
       " 670145.0656301376,\n",
       " 554278.1553381003,\n",
       " 285470.6002798154,\n",
       " 298099.89843747654,\n",
       " 569777.8971997814,\n",
       " 414472.82076040254,\n",
       " 698614.0406739789,\n",
       " 495188.6882482182,\n",
       " 946590.5654078713,\n",
       " 381195.45010471204,\n",
       " 292188.4055071451,\n",
       " 390768.6124185584,\n",
       " 288671.7563189118,\n",
       " 359956.21594553284,\n",
       " 343051.3719559573,\n",
       " 395951.88550903404,\n",
       " 915685.988397811,\n",
       " 585523.7679554528,\n",
       " 362141.9755682162,\n",
       " 508569.2687838657,\n",
       " 674793.0936064736,\n",
       " 257368.27664679097,\n",
       " 540827.4516910551,\n",
       " 367817.6681860398,\n",
       " 632763.1837917805,\n",
       " 641729.274555445,\n",
       " 464727.0333752105,\n",
       " 286695.84387804475,\n",
       " 435779.19370637153,\n",
       " 350065.0771050985,\n",
       " 870338.552322975,\n",
       " 358484.33806948736,\n",
       " 571656.7236160068,\n",
       " 414331.0061621218,\n",
       " 392937.00158175087,\n",
       " 352643.5320691647,\n",
       " 346729.3835727483,\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
