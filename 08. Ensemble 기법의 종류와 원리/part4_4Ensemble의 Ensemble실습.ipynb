{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\포스코 아카데미\\\\새 폴더\\\\전체강의자료\\\\Part 05~11) Machine Learning\\\\08. Ensemble 기법의 종류와 원리\\\\실습코드'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재경로 확인\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
       "0   1       1       0       0       0       0       0       0       0       0   \n",
       "1   2       0       0       0       0       0       0       0       1       0   \n",
       "2   3       0       0       0       0       0       0       0       1       0   \n",
       "3   4       1       0       0       1       6       1       5       0       0   \n",
       "4   5       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   ...  feat_85  feat_86  feat_87  feat_88  feat_89  feat_90  feat_91  \\\n",
       "0  ...        1        0        0        0        0        0        0   \n",
       "1  ...        0        0        0        0        0        0        0   \n",
       "2  ...        0        0        0        0        0        0        0   \n",
       "3  ...        0        1        2        0        0        0        0   \n",
       "4  ...        1        0        0        0        0        1        0   \n",
       "\n",
       "   feat_92  feat_93   target  \n",
       "0        0        0  Class_1  \n",
       "1        0        0  Class_1  \n",
       "2        0        0  Class_1  \n",
       "3        0        0  Class_1  \n",
       "4        0        0  Class_1  \n",
       "\n",
       "[5 rows x 95 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "data = pd.read_csv(\"../Data/otto_train.csv\") # Product Category\n",
    "data.head() # 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid: 고유 아이디\\nfeat_1 ~ feat_93: 설명변수\\ntarget: 타겟변수 (1~9)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "id: 고유 아이디\n",
    "feat_1 ~ feat_93: 설명변수\n",
    "target: 타겟변수 (1~9)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nCar: 61878 nVar: 95\n"
     ]
    }
   ],
   "source": [
    "nCar = data.shape[0] # 데이터 개수\n",
    "nVar = data.shape[1] # 변수 개수\n",
    "print('nCar: %d' % nCar, 'nVar: %d' % nVar )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 의미가 없다고 판단되는 변수 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['id'], axis = 1) # id 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 타겟 변수의 문자열을 숫자로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {\"Class_1\": 1,\n",
    "                \"Class_2\": 2,\n",
    "                \"Class_3\": 3,\n",
    "                \"Class_4\": 4,\n",
    "                \"Class_5\": 5,\n",
    "                \"Class_6\": 6,\n",
    "                \"Class_7\": 7,\n",
    "                \"Class_8\": 8,\n",
    "                \"Class_9\": 9}\n",
    "after_mapping_target = data['target'].apply(lambda x: mapping_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설명변수와 타겟변수를 분리, 학습데이터와 평가데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49502, 93) (12376, 93) (49502,) (12376,)\n"
     ]
    }
   ],
   "source": [
    "feature_columns = list(data.columns.difference(['target'])) # target을 제외한 모든 행\n",
    "X = data[feature_columns] # 설명변수\n",
    "y = after_mapping_target # 타겟변수\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 42) # 학습데이터와 평가데이터의 비율을 8:2 로 분할| \n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape) # 데이터 개수 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:48:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:48:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 76.67 %\n",
      "Time: 4.71 seconds\n"
     ]
    }
   ],
   "source": [
    "# !pip install xgboost\n",
    "import xgboost as xgb\n",
    "import time\n",
    "start = time.time() # 시작 시간 지정\n",
    "xgb_dtrain = xgb.DMatrix(data = train_x, label = train_y) # 학습 데이터를 XGBoost 모델에 맞게 변환\n",
    "xgb_dtest = xgb.DMatrix(data = test_x) # 평가 데이터를 XGBoost 모델에 맞게 변환\n",
    "xgb_param = {'max_depth': 10, # 트리 깊이\n",
    "         'learning_rate': 0.01, # Step Size\n",
    "         'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "         'objective': 'multi:softmax', # 목적 함수\n",
    "        'num_class': len(set(train_y)) + 1} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "xgb_model = xgb.train(params = xgb_param, dtrain = xgb_dtrain) # 학습 진행\n",
    "xgb_model_predict = xgb_model.predict(xgb_dtest) # 평가 데이터 예측\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, xgb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 3., 6., ..., 9., 2., 7.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013198 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3110\n",
      "[LightGBM] [Info] Number of data points in the train set: 49502, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -3.476745\n",
      "[LightGBM] [Info] Start training from score -1.341381\n",
      "[LightGBM] [Info] Start training from score -2.039019\n",
      "[LightGBM] [Info] Start training from score -3.135151\n",
      "[LightGBM] [Info] Start training from score -3.125444\n",
      "[LightGBM] [Info] Start training from score -1.481556\n",
      "[LightGBM] [Info] Start training from score -3.074772\n",
      "[LightGBM] [Info] Start training from score -1.986562\n",
      "[LightGBM] [Info] Start training from score -2.533374\n",
      "Accuracy: 76.28 %\n",
      "Time: 1.83 seconds\n"
     ]
    }
   ],
   "source": [
    "# !pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "start = time.time() # 시작 시간 지정\n",
    "lgb_dtrain = lgb.Dataset(data = train_x, label = train_y) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "lgb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "            'objective': 'multiclass', # 목적 함수\n",
    "            'num_class': len(set(train_y)) + 1} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n",
    "lgb_model_predict = np.argmax(lgb_model.predict(test_x), axis = 1) # 평가 데이터 예측, Softmax의 결과값 중 가장 큰 값의 Label로 예측\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, lgb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.01734061e-15, 2.25081693e-02, 3.62193933e-01, ...,\n",
       "        3.24234521e-02, 5.82126692e-02, 3.67722414e-02],\n",
       "       [1.14084116e-15, 5.36978636e-02, 1.90687128e-01, ...,\n",
       "        3.25081119e-01, 9.38028846e-02, 6.50463131e-02],\n",
       "       [5.94595781e-16, 9.66842220e-03, 5.82817482e-02, ...,\n",
       "        1.42318289e-02, 3.40230275e-02, 2.14919364e-02],\n",
       "       ...,\n",
       "       [7.09105769e-16, 4.63740004e-02, 1.08297559e-01, ...,\n",
       "        5.46934960e-02, 7.24513712e-02, 5.74635996e-01],\n",
       "       [9.88127136e-16, 1.54895684e-02, 5.45515599e-01, ...,\n",
       "        2.45870954e-02, 5.65410617e-02, 3.62344513e-02],\n",
       "       [7.59617500e-16, 1.49480877e-02, 7.44570300e-02, ...,\n",
       "        5.76695793e-01, 1.43227106e-01, 2.74567219e-02]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5907034\ttotal: 523ms\tremaining: 51.8s\n",
      "1:\tlearn: 0.6356107\ttotal: 1.05s\tremaining: 51.4s\n",
      "2:\tlearn: 0.6411256\ttotal: 1.57s\tremaining: 50.8s\n",
      "3:\tlearn: 0.6480344\ttotal: 2.1s\tremaining: 50.3s\n",
      "4:\tlearn: 0.6508222\ttotal: 2.62s\tremaining: 49.8s\n",
      "5:\tlearn: 0.6499939\ttotal: 3.14s\tremaining: 49.2s\n",
      "6:\tlearn: 0.6507818\ttotal: 3.65s\tremaining: 48.5s\n",
      "7:\tlearn: 0.6548422\ttotal: 4.17s\tremaining: 48s\n",
      "8:\tlearn: 0.6559533\ttotal: 4.69s\tremaining: 47.4s\n",
      "9:\tlearn: 0.6560947\ttotal: 5.21s\tremaining: 46.8s\n",
      "10:\tlearn: 0.6568421\ttotal: 5.72s\tremaining: 46.3s\n",
      "11:\tlearn: 0.6588219\ttotal: 6.25s\tremaining: 45.9s\n",
      "12:\tlearn: 0.6592259\ttotal: 6.77s\tremaining: 45.3s\n",
      "13:\tlearn: 0.6611248\ttotal: 7.29s\tremaining: 44.8s\n",
      "14:\tlearn: 0.6625591\ttotal: 7.81s\tremaining: 44.2s\n",
      "15:\tlearn: 0.6631853\ttotal: 8.33s\tremaining: 43.7s\n",
      "16:\tlearn: 0.6639328\ttotal: 8.85s\tremaining: 43.2s\n",
      "17:\tlearn: 0.6668821\ttotal: 9.38s\tremaining: 42.7s\n",
      "18:\tlearn: 0.6669630\ttotal: 9.89s\tremaining: 42.2s\n",
      "19:\tlearn: 0.6675286\ttotal: 10.4s\tremaining: 41.6s\n",
      "20:\tlearn: 0.6673266\ttotal: 10.9s\tremaining: 41.1s\n",
      "21:\tlearn: 0.6677104\ttotal: 11.5s\tremaining: 40.7s\n",
      "22:\tlearn: 0.6682558\ttotal: 12s\tremaining: 40.2s\n",
      "23:\tlearn: 0.6683972\ttotal: 12.5s\tremaining: 39.7s\n",
      "24:\tlearn: 0.6686599\ttotal: 13.1s\tremaining: 39.1s\n",
      "25:\tlearn: 0.6681952\ttotal: 13.6s\tremaining: 38.6s\n",
      "26:\tlearn: 0.6684982\ttotal: 14.1s\tremaining: 38.1s\n",
      "27:\tlearn: 0.6692053\ttotal: 14.6s\tremaining: 37.6s\n",
      "28:\tlearn: 0.6696699\ttotal: 15.1s\tremaining: 37.1s\n",
      "29:\tlearn: 0.6699325\ttotal: 15.7s\tremaining: 36.6s\n",
      "30:\tlearn: 0.6705992\ttotal: 16.2s\tremaining: 36s\n",
      "31:\tlearn: 0.6709426\ttotal: 16.7s\tremaining: 35.5s\n",
      "32:\tlearn: 0.6708012\ttotal: 17.2s\tremaining: 35s\n",
      "33:\tlearn: 0.6709426\ttotal: 17.7s\tremaining: 34.4s\n",
      "34:\tlearn: 0.6707002\ttotal: 18.3s\tremaining: 33.9s\n",
      "35:\tlearn: 0.6715082\ttotal: 18.9s\tremaining: 33.5s\n",
      "36:\tlearn: 0.6705992\ttotal: 19.4s\tremaining: 33s\n",
      "37:\tlearn: 0.6725991\ttotal: 19.9s\tremaining: 32.5s\n",
      "38:\tlearn: 0.6729829\ttotal: 20.4s\tremaining: 32s\n",
      "39:\tlearn: 0.6725991\ttotal: 21s\tremaining: 31.5s\n",
      "40:\tlearn: 0.6734273\ttotal: 21.5s\tremaining: 31s\n",
      "41:\tlearn: 0.6738314\ttotal: 22.1s\tremaining: 30.5s\n",
      "42:\tlearn: 0.6741546\ttotal: 22.6s\tremaining: 30s\n",
      "43:\tlearn: 0.6739728\ttotal: 23.3s\tremaining: 29.6s\n",
      "44:\tlearn: 0.6741950\ttotal: 23.8s\tremaining: 29.1s\n",
      "45:\tlearn: 0.6750636\ttotal: 24.3s\tremaining: 28.6s\n",
      "46:\tlearn: 0.6758919\ttotal: 24.9s\tremaining: 28s\n",
      "47:\tlearn: 0.6757707\ttotal: 25.4s\tremaining: 27.5s\n",
      "48:\tlearn: 0.6762151\ttotal: 25.9s\tremaining: 27s\n",
      "49:\tlearn: 0.6774474\ttotal: 26.5s\tremaining: 26.5s\n",
      "50:\tlearn: 0.6777100\ttotal: 27.1s\tremaining: 26s\n",
      "51:\tlearn: 0.6786594\ttotal: 27.7s\tremaining: 25.5s\n",
      "52:\tlearn: 0.6789827\ttotal: 28.2s\tremaining: 25s\n",
      "53:\tlearn: 0.6804372\ttotal: 28.8s\tremaining: 24.5s\n",
      "54:\tlearn: 0.6804372\ttotal: 29.3s\tremaining: 24s\n",
      "55:\tlearn: 0.6809220\ttotal: 29.9s\tremaining: 23.5s\n",
      "56:\tlearn: 0.6812250\ttotal: 30.5s\tremaining: 23s\n",
      "57:\tlearn: 0.6813058\ttotal: 31.1s\tremaining: 22.5s\n",
      "58:\tlearn: 0.6811846\ttotal: 31.6s\tremaining: 22s\n",
      "59:\tlearn: 0.6813260\ttotal: 32.2s\tremaining: 21.5s\n",
      "60:\tlearn: 0.6816694\ttotal: 32.8s\tremaining: 20.9s\n",
      "61:\tlearn: 0.6823159\ttotal: 33.3s\tremaining: 20.4s\n",
      "62:\tlearn: 0.6832653\ttotal: 33.9s\tremaining: 19.9s\n",
      "63:\tlearn: 0.6840734\ttotal: 34.4s\tremaining: 19.4s\n",
      "64:\tlearn: 0.6840734\ttotal: 35s\tremaining: 18.8s\n",
      "65:\tlearn: 0.6846592\ttotal: 35.5s\tremaining: 18.3s\n",
      "66:\tlearn: 0.6843360\ttotal: 36.1s\tremaining: 17.8s\n",
      "67:\tlearn: 0.6846390\ttotal: 36.6s\tremaining: 17.2s\n",
      "68:\tlearn: 0.6854269\ttotal: 37.2s\tremaining: 16.7s\n",
      "69:\tlearn: 0.6858309\ttotal: 37.7s\tremaining: 16.2s\n",
      "70:\tlearn: 0.6858309\ttotal: 38.2s\tremaining: 15.6s\n",
      "71:\tlearn: 0.6865783\ttotal: 38.8s\tremaining: 15.1s\n",
      "72:\tlearn: 0.6864167\ttotal: 39.3s\tremaining: 14.5s\n",
      "73:\tlearn: 0.6868611\ttotal: 39.9s\tremaining: 14s\n",
      "74:\tlearn: 0.6869217\ttotal: 40.4s\tremaining: 13.5s\n",
      "75:\tlearn: 0.6870429\ttotal: 41s\tremaining: 12.9s\n",
      "76:\tlearn: 0.6875278\ttotal: 41.5s\tremaining: 12.4s\n",
      "77:\tlearn: 0.6881136\ttotal: 42.1s\tremaining: 11.9s\n",
      "78:\tlearn: 0.6883762\ttotal: 42.6s\tremaining: 11.3s\n",
      "79:\tlearn: 0.6888207\ttotal: 43.1s\tremaining: 10.8s\n",
      "80:\tlearn: 0.6892449\ttotal: 43.7s\tremaining: 10.2s\n",
      "81:\tlearn: 0.6898509\ttotal: 44.2s\tremaining: 9.71s\n",
      "82:\tlearn: 0.6897095\ttotal: 44.8s\tremaining: 9.17s\n",
      "83:\tlearn: 0.6902549\ttotal: 45.3s\tremaining: 8.63s\n",
      "84:\tlearn: 0.6909822\ttotal: 45.9s\tremaining: 8.09s\n",
      "85:\tlearn: 0.6910832\ttotal: 46.4s\tremaining: 7.56s\n",
      "86:\tlearn: 0.6914468\ttotal: 47s\tremaining: 7.02s\n",
      "87:\tlearn: 0.6916084\ttotal: 47.5s\tremaining: 6.48s\n",
      "88:\tlearn: 0.6919922\ttotal: 48.1s\tremaining: 5.94s\n",
      "89:\tlearn: 0.6925579\ttotal: 48.6s\tremaining: 5.4s\n",
      "90:\tlearn: 0.6928407\ttotal: 49.2s\tremaining: 4.86s\n",
      "91:\tlearn: 0.6930427\ttotal: 49.7s\tremaining: 4.32s\n",
      "92:\tlearn: 0.6935073\ttotal: 50.2s\tremaining: 3.78s\n",
      "93:\tlearn: 0.6940932\ttotal: 50.8s\tremaining: 3.24s\n",
      "94:\tlearn: 0.6944972\ttotal: 51.4s\tremaining: 2.7s\n",
      "95:\tlearn: 0.6948810\ttotal: 51.9s\tremaining: 2.16s\n",
      "96:\tlearn: 0.6951840\ttotal: 52.4s\tremaining: 1.62s\n",
      "97:\tlearn: 0.6954264\ttotal: 53s\tremaining: 1.08s\n",
      "98:\tlearn: 0.6955881\ttotal: 53.5s\tremaining: 541ms\n",
      "99:\tlearn: 0.6956285\ttotal: 54.1s\tremaining: 0us\n",
      "Accuracy: 69.64 %\n",
      "Time: 54.30 seconds\n"
     ]
    }
   ],
   "source": [
    "# !pip install catboost\n",
    "import catboost as cb\n",
    "start = time.time() # 시작 시간 지정\n",
    "cb_dtrain = cb.Pool(data = train_x, label = train_y) # 학습 데이터를 Catboost 모델에 맞게 변환\n",
    "cb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "            'eval_metric': 'Accuracy', # 평가 척도\n",
    "            'loss_function': 'MultiClass'} # 손실 함수, 목적 함수\n",
    "cb_model = cb.train(pool = cb_dtrain, params = cb_param) # 학습 진행\n",
    "cb_model_predict = np.argmax(cb_model.predict(test_x), axis = 1) + 1 # 평가 데이터 예측, Softmax의 결과값 중 가장 큰 값의 Label로 예측, 인덱스의 순서를 맞추기 위해 +1\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, cb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.35426047,  1.22109587,  0.44230101, ..., -0.1698448 ,\n",
       "        -0.02059177, -0.2130643 ],\n",
       "       [-0.07235138,  0.42535181,  0.20060428, ...,  0.21863604,\n",
       "         0.2719157 ,  0.25089315],\n",
       "       [-0.3315885 , -0.31862353, -0.31279765, ..., -0.29798357,\n",
       "        -0.24018767, -0.32984969],\n",
       "       ...,\n",
       "       [ 0.05304325,  0.02500267, -0.14752573, ..., -0.20741963,\n",
       "         0.12789417,  1.51166757],\n",
       "       [-0.55093666,  1.7691278 ,  0.99746884, ..., -0.3420542 ,\n",
       "        -0.49799871, -0.38136323],\n",
       "       [-0.3033724 ,  0.09352675, -0.11808658, ...,  0.65825036,\n",
       "         1.05515787, -0.20799899]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  floors  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00     1.0   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25     2.0   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00     1.0   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00     1.0   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00     1.0   \n",
       "\n",
       "   waterfront  condition  grade  yr_built  yr_renovated  zipcode      lat  \\\n",
       "0           0          3      7      1955             0    98178  47.5112   \n",
       "1           0          3      7      1951          1991    98125  47.7210   \n",
       "2           0          3      6      1933             0    98028  47.7379   \n",
       "3           0          5      7      1965             0    98136  47.5208   \n",
       "4           0          3      8      1987             0    98074  47.6168   \n",
       "\n",
       "      long  \n",
       "0 -122.257  \n",
       "1 -122.319  \n",
       "2 -122.233  \n",
       "3 -122.393  \n",
       "4 -122.045  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "data = pd.read_csv(\"../Data/kc_house_data.csv\") \n",
    "data.head() # 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['id', 'date', 'zipcode', 'lat', 'long'], axis = 1) # id, date, zipcode, lat, long  제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15129, 8) (6484, 8) (15129,) (6484,)\n"
     ]
    }
   ],
   "source": [
    "feature_columns = list(data.columns.difference(['price'])) # Price를 제외한 모든 행\n",
    "X = data[feature_columns]\n",
    "y = data['price']\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.3, random_state = 42) # 학습데이터와 평가데이터의 비율을 7:3\n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape) # 데이터 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000205 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 237\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537729.263666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "# !pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "start = time.time() # 시작 시간 지정\n",
    "lgb_dtrain = lgb.Dataset(data = train_x, label = train_y) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "lgb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 500, # Number of trees, 트리 생성 개수\n",
    "            'objective': 'regression'} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210904.17249451784"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "sqrt(mean_squared_error(lgb_model.predict(test_x),test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble의 Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9538\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000269 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 539287.872364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "224634.27660031084\n",
      "9615\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 235\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 538238.073039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209959.04928244164\n",
      "9551\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000194 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 539687.810166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218241.94622134304\n",
      "9581\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000197 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 534871.213629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214965.21638446985\n",
      "9559\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000308 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 234\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 539880.272391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209865.11404486664\n",
      "9601\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000278 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 234\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537073.267962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211110.81944760346\n",
      "9579\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 236\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537597.839514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214076.59968665396\n",
      "9587\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 238\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 532952.099478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "216485.80279064312\n",
      "9574\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 236\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 535314.833763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "212877.73977264567\n",
      "9579\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000196 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 230\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 535658.501884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212683.45611978348\n",
      "9622\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 236\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 536744.308547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "210359.01202411533\n",
      "9508\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000204 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 234\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 539266.775729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215049.77393902699\n",
      "9526\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537036.259700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214730.8624651196\n",
      "9569\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 536834.147531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213485.3951968252\n",
      "9595\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 535153.821204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214508.1547985367\n",
      "9542\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000418 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 230\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 535674.240862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211866.66039393432\n",
      "9551\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 543726.785313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215406.09261490413\n",
      "9518\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 532843.583978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216892.50507223824\n",
      "9554\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 540388.278141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215800.8084379064\n",
      "9534\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 540179.526340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216077.25527047267\n",
      "9539\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000192 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 229\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 539928.461167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214095.5250908152\n",
      "9547\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000194 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 235\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 546033.416948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "217381.12467735258\n",
      "9591\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 230\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 533767.766343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "215193.09622395656\n",
      "9590\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 234\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 532837.232930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215703.2894008167\n",
      "9531\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 234\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537304.802829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218869.7054313527\n",
      "9648\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 536008.775464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214443.20252628732\n",
      "9602\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 235\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 539300.863970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213558.20269827291\n",
      "9572\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000197 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 535366.171194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "213626.83124523683\n",
      "9560\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000194 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 235\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 536941.644590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212781.82969962028\n",
      "9607\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000250 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 540107.725692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82105\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "220595.63953914208\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "bagging_predict_result = [] # 빈 리스트 생성\n",
    "for _ in range(30):\n",
    "    data_index = [data_index for data_index in range(train_x.shape[0])] # 학습 데이터의 인덱스를 리스트로 변환\n",
    "    random_data_index = np.random.choice(data_index, train_x.shape[0]) # 데이터의 1/10 크기만큼 랜덤 샘플링, // 는 소수점을 무시하기 위함\n",
    "    print(len(set(random_data_index)))\n",
    "    lgb_dtrain = lgb.Dataset(data = train_x.iloc[random_data_index,], label = train_y.iloc[random_data_index]) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "    lgb_param = {'max_depth': 10, # 트리 깊이\n",
    "                'learning_rate': 0.01, # Step Size\n",
    "                'n_estimators': 500, # Number of trees, 트리 생성 개수\n",
    "                'objective': 'regression'} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "    lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n",
    " \n",
    "    predict1 = lgb_model.predict(test_x) # 테스트 데이터 예측\n",
    "    bagging_predict_result.append(predict1) # 반복문이 실행되기 전 빈 리스트에 결과 값 저장\n",
    "    print(sqrt(mean_squared_error(lgb_model.predict(test_x),test_y)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging을 바탕으로 예측한 결과값에 대한 평균을 계산\n",
    "bagging_predict = [] # 빈 리스트 생성\n",
    "for lst2_index in range(test_x.shape[0]): # 테스트 데이터 개수만큼의 반복\n",
    "    temp_predict = [] # 임시 빈 리스트 생성 (반복문 내 결과값 저장)\n",
    "    for lst_index in range(len(bagging_predict_result)): # Bagging 결과 리스트 반복\n",
    "        temp_predict.append(bagging_predict_result[lst_index][lst2_index]) # 각 Bagging 결과 예측한 값 중 같은 인덱스를 리스트에 저장\n",
    "    bagging_predict.append(np.mean(temp_predict)) # 해당 인덱스의 30개의 결과값에 대한 평균을 최종 리스트에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[511968.70762444,\n",
       " 630670.4705015719,\n",
       " 957160.3682093942,\n",
       " 1597626.9514123963,\n",
       " 638513.0614077034,\n",
       " 369141.98957375303,\n",
       " 709074.1824429895,\n",
       " 430563.16652366816,\n",
       " 463206.2069760023,\n",
       " 495325.0371860035,\n",
       " 635820.8626711474,\n",
       " 379930.862689403,\n",
       " 299543.4574700815,\n",
       " 359486.8774971494,\n",
       " 343454.2017378641,\n",
       " 1344409.6475217298,\n",
       " 368585.1214884795,\n",
       " 1042334.8071794773,\n",
       " 316790.2213904472,\n",
       " 530498.3516233434,\n",
       " 377708.4758691121,\n",
       " 1948797.563707676,\n",
       " 665405.8984293451,\n",
       " 547596.1275632001,\n",
       " 502252.9751018808,\n",
       " 486130.11918997054,\n",
       " 296824.4823836274,\n",
       " 253675.68706659632,\n",
       " 474571.0410481569,\n",
       " 534513.0094234912,\n",
       " 487900.5501061199,\n",
       " 474050.42831432837,\n",
       " 467176.45945790655,\n",
       " 570328.074834467,\n",
       " 377943.62772537203,\n",
       " 1047127.86503995,\n",
       " 879134.1277714131,\n",
       " 531831.0300925743,\n",
       " 356815.44597666466,\n",
       " 1565522.6638948868,\n",
       " 391942.95602817653,\n",
       " 276467.05852488906,\n",
       " 506106.53918424365,\n",
       " 341750.0097026846,\n",
       " 254834.59486977558,\n",
       " 242614.17978034663,\n",
       " 328216.4263006419,\n",
       " 333321.46332013834,\n",
       " 355326.3512981341,\n",
       " 566155.8996705321,\n",
       " 369468.36938127293,\n",
       " 339588.3344857821,\n",
       " 772717.2955808622,\n",
       " 333920.1144890692,\n",
       " 464447.9013170713,\n",
       " 1701904.8266502745,\n",
       " 480344.6402237853,\n",
       " 708697.8687411471,\n",
       " 334582.6144271886,\n",
       " 647160.9844931625,\n",
       " 477395.6873700413,\n",
       " 374432.0939745389,\n",
       " 297953.65626436175,\n",
       " 526642.535369093,\n",
       " 452618.9845891232,\n",
       " 285892.8781673476,\n",
       " 388945.60609698005,\n",
       " 1570367.097081542,\n",
       " 485127.6998785873,\n",
       " 654934.0255444667,\n",
       " 432686.8413521013,\n",
       " 299389.73843611794,\n",
       " 764397.7737803495,\n",
       " 521561.0843801886,\n",
       " 513402.2956099687,\n",
       " 1312586.168575035,\n",
       " 820906.8482880042,\n",
       " 288933.28537472273,\n",
       " 455178.116726312,\n",
       " 907721.7860135718,\n",
       " 634948.1305503343,\n",
       " 376476.67596211855,\n",
       " 655080.2287201401,\n",
       " 360886.8901265182,\n",
       " 826168.845501938,\n",
       " 523239.3922338393,\n",
       " 526949.9067909835,\n",
       " 557531.5904196876,\n",
       " 359060.3321993469,\n",
       " 462890.9165068657,\n",
       " 349607.6418471875,\n",
       " 396913.6515734378,\n",
       " 631376.2684606153,\n",
       " 1088241.2745953535,\n",
       " 430873.73226849816,\n",
       " 495343.11021859455,\n",
       " 360878.35947213613,\n",
       " 307143.34503531526,\n",
       " 813816.4060474427,\n",
       " 460492.0673602612,\n",
       " 257598.7288702025,\n",
       " 920804.0824356016,\n",
       " 1001534.9610561184,\n",
       " 478896.4907528451,\n",
       " 1063041.8927138958,\n",
       " 298631.05256462767,\n",
       " 491179.11600800976,\n",
       " 483451.3311168506,\n",
       " 814235.0813229899,\n",
       " 2318876.639205692,\n",
       " 550670.861582972,\n",
       " 323329.0761266233,\n",
       " 557258.9723173145,\n",
       " 623362.6527248747,\n",
       " 551740.4240361425,\n",
       " 333950.6443720968,\n",
       " 311434.4809597528,\n",
       " 254004.6308040895,\n",
       " 321195.31353433593,\n",
       " 343454.2017378641,\n",
       " 380911.6702708464,\n",
       " 284597.2368562262,\n",
       " 342265.78397315124,\n",
       " 256714.7566705568,\n",
       " 594879.3435667196,\n",
       " 656991.0352145365,\n",
       " 277524.2039634323,\n",
       " 750085.4508736929,\n",
       " 452947.82471902866,\n",
       " 424119.4765358939,\n",
       " 534438.1205973314,\n",
       " 463906.4638928686,\n",
       " 408143.667910616,\n",
       " 831944.5801022644,\n",
       " 377474.0403575214,\n",
       " 460329.786676522,\n",
       " 386241.2146499435,\n",
       " 348523.96533419384,\n",
       " 901129.789871095,\n",
       " 619444.6824427188,\n",
       " 516130.53254371596,\n",
       " 782896.0438927094,\n",
       " 910473.1067790559,\n",
       " 405629.191327406,\n",
       " 258338.40550182763,\n",
       " 383120.3461782089,\n",
       " 483881.7479107193,\n",
       " 243541.90591285875,\n",
       " 412205.51370939624,\n",
       " 472070.994789854,\n",
       " 574086.3505421153,\n",
       " 678688.7975589446,\n",
       " 552572.2186135688,\n",
       " 1110062.608972908,\n",
       " 897307.5297607273,\n",
       " 862661.0314458652,\n",
       " 583114.2746388504,\n",
       " 651970.4965845925,\n",
       " 582274.7891079747,\n",
       " 491423.91582144785,\n",
       " 643943.3552230506,\n",
       " 370064.39032829617,\n",
       " 333321.46332013834,\n",
       " 357507.8666035989,\n",
       " 362168.0375511009,\n",
       " 344047.1073467349,\n",
       " 283488.05009550473,\n",
       " 311275.8313487362,\n",
       " 450020.8647861266,\n",
       " 464068.45398931555,\n",
       " 624278.1455625894,\n",
       " 397393.40263039886,\n",
       " 463940.21144574304,\n",
       " 580034.057645758,\n",
       " 425445.0158270608,\n",
       " 413765.1105736987,\n",
       " 358305.9402330494,\n",
       " 676890.2015076958,\n",
       " 347047.53060258855,\n",
       " 256235.92991107696,\n",
       " 311616.3854899923,\n",
       " 476336.4191453565,\n",
       " 537921.3828216451,\n",
       " 673333.6597415146,\n",
       " 470609.74488402554,\n",
       " 469772.57151837164,\n",
       " 274600.0206798885,\n",
       " 427062.9740838909,\n",
       " 352254.68321680184,\n",
       " 350718.56051868026,\n",
       " 372250.2730419343,\n",
       " 648959.0411504194,\n",
       " 1546871.7649680478,\n",
       " 1278695.1737010316,\n",
       " 266142.3005719368,\n",
       " 488697.08899247664,\n",
       " 492446.88787078636,\n",
       " 1636256.3773482584,\n",
       " 458529.64295323915,\n",
       " 459969.331349644,\n",
       " 325192.2895566766,\n",
       " 384333.351298599,\n",
       " 523587.95673143736,\n",
       " 770493.4946858431,\n",
       " 790568.5656328868,\n",
       " 312777.36948924884,\n",
       " 502252.9751018808,\n",
       " 306655.31181281246,\n",
       " 509887.52221757645,\n",
       " 1368101.562624596,\n",
       " 360878.35947213613,\n",
       " 420834.8973195215,\n",
       " 456648.6060543017,\n",
       " 360878.35947213613,\n",
       " 331403.4528576524,\n",
       " 698453.440220832,\n",
       " 800709.647258232,\n",
       " 347821.901064454,\n",
       " 363520.3838743204,\n",
       " 360255.0316618023,\n",
       " 1689038.6312881254,\n",
       " 534642.4918062725,\n",
       " 503064.9623391535,\n",
       " 453741.1728906831,\n",
       " 523760.6535282873,\n",
       " 752795.4424052457,\n",
       " 344994.3495245949,\n",
       " 1259866.162859254,\n",
       " 881505.245190914,\n",
       " 462700.01374879555,\n",
       " 357001.4555858317,\n",
       " 483396.92690993287,\n",
       " 713896.9392199387,\n",
       " 299537.0035124962,\n",
       " 344571.5758756875,\n",
       " 395451.8221418621,\n",
       " 347669.58746839757,\n",
       " 358853.00503934635,\n",
       " 2483038.5510270544,\n",
       " 343194.319968695,\n",
       " 441789.7712021962,\n",
       " 461905.2221349729,\n",
       " 595162.9070391807,\n",
       " 413898.2707601778,\n",
       " 468839.5617316934,\n",
       " 309164.4582657146,\n",
       " 525898.1648609528,\n",
       " 555606.4126617936,\n",
       " 722599.9609346924,\n",
       " 868577.9659306373,\n",
       " 536898.2243616023,\n",
       " 435374.13992529124,\n",
       " 720468.3303687162,\n",
       " 351153.3310521824,\n",
       " 350718.56051868026,\n",
       " 527292.719848242,\n",
       " 500133.9131082973,\n",
       " 465349.7068525311,\n",
       " 888215.3960064019,\n",
       " 370847.7189264049,\n",
       " 3214095.9135081214,\n",
       " 633674.7124415013,\n",
       " 762094.1110753225,\n",
       " 1048251.9082253077,\n",
       " 508061.65533418546,\n",
       " 675356.3910868411,\n",
       " 920427.3906108172,\n",
       " 349093.8104167593,\n",
       " 725352.4198193739,\n",
       " 435863.17465863616,\n",
       " 473320.47414601385,\n",
       " 342323.54327673523,\n",
       " 290053.75684849947,\n",
       " 438391.6823337396,\n",
       " 414782.3359333221,\n",
       " 1389651.6796628172,\n",
       " 304344.4570049219,\n",
       " 344708.8686117509,\n",
       " 530256.9870733551,\n",
       " 362295.6181559573,\n",
       " 311151.89902846026,\n",
       " 507339.0334189208,\n",
       " 369038.3312560833,\n",
       " 442317.5585246195,\n",
       " 483940.9035017754,\n",
       " 451349.1270324214,\n",
       " 370782.46323399816,\n",
       " 633271.6485569013,\n",
       " 353773.14132450585,\n",
       " 317989.99723954184,\n",
       " 779544.9109541235,\n",
       " 445826.9441049454,\n",
       " 259275.56323077806,\n",
       " 348520.33726383,\n",
       " 648959.0411504194,\n",
       " 649920.083910173,\n",
       " 493484.85842175543,\n",
       " 446163.977028543,\n",
       " 453550.0645960589,\n",
       " 577653.3542731781,\n",
       " 471776.6633007364,\n",
       " 543657.1817218428,\n",
       " 333802.6578159121,\n",
       " 576306.3968927179,\n",
       " 343756.26612438465,\n",
       " 822881.8567181473,\n",
       " 450020.8647861266,\n",
       " 387275.85839398974,\n",
       " 333065.5676714593,\n",
       " 330518.83102408756,\n",
       " 367777.13416693726,\n",
       " 291818.28397755616,\n",
       " 858541.0885805987,\n",
       " 1619797.7326101908,\n",
       " 966739.207719675,\n",
       " 448605.7591420178,\n",
       " 812374.3988528046,\n",
       " 464607.40110543993,\n",
       " 798381.2856471711,\n",
       " 345847.51603873464,\n",
       " 403508.7349208532,\n",
       " 491820.7023882485,\n",
       " 266142.3005719368,\n",
       " 299494.7143926315,\n",
       " 464135.6834319989,\n",
       " 464068.45398931555,\n",
       " 568561.7443640159,\n",
       " 299165.2340494793,\n",
       " 509841.31044902006,\n",
       " 256714.7566705568,\n",
       " 662663.3280318304,\n",
       " 295535.3581280823,\n",
       " 499860.99096628075,\n",
       " 300743.9198781001,\n",
       " 390587.71740241966,\n",
       " 479495.1054218737,\n",
       " 608968.4183890616,\n",
       " 472536.6877225781,\n",
       " 930430.7629085002,\n",
       " 261023.57206135723,\n",
       " 1723716.3914542168,\n",
       " 473952.90893462976,\n",
       " 441121.37671518384,\n",
       " 571290.3550659345,\n",
       " 679849.4389769423,\n",
       " 619899.7354864583,\n",
       " 342803.9170593238,\n",
       " 461759.64436867175,\n",
       " 477115.9098259014,\n",
       " 712934.7576660594,\n",
       " 283221.3017470241,\n",
       " 365560.93437314965,\n",
       " 473285.60879691417,\n",
       " 612503.753043798,\n",
       " 341251.55703769415,\n",
       " 856888.1887013736,\n",
       " 556485.7157031724,\n",
       " 916241.3229537846,\n",
       " 985281.4118461206,\n",
       " 636695.2318646491,\n",
       " 370787.028121534,\n",
       " 797511.6896380082,\n",
       " 357913.2519583398,\n",
       " 565762.5160721618,\n",
       " 685675.5848504172,\n",
       " 331315.592516896,\n",
       " 767167.6085285811,\n",
       " 402487.2179324651,\n",
       " 972828.1459761058,\n",
       " 387075.17811950855,\n",
       " 500083.18589999806,\n",
       " 663436.3267212884,\n",
       " 584070.4516194871,\n",
       " 354395.7334257701,\n",
       " 545297.6382429631,\n",
       " 379928.1572690093,\n",
       " 340692.6334226525,\n",
       " 550473.0563943988,\n",
       " 388560.1935917188,\n",
       " 418285.97457948903,\n",
       " 371269.5023998598,\n",
       " 720124.5718933918,\n",
       " 640939.2978900723,\n",
       " 436922.73125817586,\n",
       " 462631.7026174662,\n",
       " 459041.2994871707,\n",
       " 300555.10550937994,\n",
       " 494481.74478003546,\n",
       " 427328.4525133555,\n",
       " 464159.07227048214,\n",
       " 566091.6531339535,\n",
       " 389860.11160988995,\n",
       " 378826.2625158106,\n",
       " 408052.98265153164,\n",
       " 1422731.0578726581,\n",
       " 387075.17811950855,\n",
       " 345243.84957498935,\n",
       " 1212765.694382599,\n",
       " 666104.6130491373,\n",
       " 386942.51669787965,\n",
       " 432686.8413521013,\n",
       " 474906.0911146842,\n",
       " 660021.9280990525,\n",
       " 456161.9479655617,\n",
       " 394194.656273691,\n",
       " 438285.3392403428,\n",
       " 464068.45398931555,\n",
       " 424119.4765358939,\n",
       " 483116.65553656965,\n",
       " 470034.7983171948,\n",
       " 351169.39927103673,\n",
       " 474035.48540634586,\n",
       " 612878.2985684983,\n",
       " 423165.7606356133,\n",
       " 268926.90708355163,\n",
       " 383817.7409393738,\n",
       " 462854.66542673256,\n",
       " 450938.0372209853,\n",
       " 1096510.1372154092,\n",
       " 454877.88424348703,\n",
       " 394375.00793090666,\n",
       " 562138.0523086507,\n",
       " 332418.0767320196,\n",
       " 662778.4964184916,\n",
       " 404249.3015620246,\n",
       " 475257.68379229284,\n",
       " 485823.53047694504,\n",
       " 470234.8769524636,\n",
       " 555673.1350969677,\n",
       " 333940.5681323487,\n",
       " 602474.7841840022,\n",
       " 513727.80535074574,\n",
       " 694972.0551110425,\n",
       " 533278.6988585716,\n",
       " 510468.89437452523,\n",
       " 496214.8718820739,\n",
       " 521555.488018499,\n",
       " 382474.5580491702,\n",
       " 363913.1769450183,\n",
       " 349969.9706993338,\n",
       " 647098.1114474199,\n",
       " 384033.0028791854,\n",
       " 360660.3367833152,\n",
       " 273671.0376889705,\n",
       " 367147.6070138056,\n",
       " 386770.2261792252,\n",
       " 831944.5801022644,\n",
       " 2360542.7466805023,\n",
       " 446337.28881471034,\n",
       " 1181328.830882031,\n",
       " 500354.705026427,\n",
       " 267452.06526462024,\n",
       " 482405.6040113017,\n",
       " 449953.0982875531,\n",
       " 433770.968733297,\n",
       " 668353.557722862,\n",
       " 379941.2086859786,\n",
       " 827081.537768223,\n",
       " 372962.9417029822,\n",
       " 277447.5788456113,\n",
       " 464188.86153859465,\n",
       " 635752.0047918113,\n",
       " 997274.1508134624,\n",
       " 374246.0873798279,\n",
       " 726261.1187892625,\n",
       " 452803.4942691295,\n",
       " 357401.50392781605,\n",
       " 540651.3879410407,\n",
       " 985021.5546689915,\n",
       " 435361.5610719632,\n",
       " 472536.6877225781,\n",
       " 351169.39927103673,\n",
       " 455470.03916061355,\n",
       " 1194585.5316760375,\n",
       " 445211.4651432497,\n",
       " 496243.6706557904,\n",
       " 388886.8181086009,\n",
       " 459118.3603825078,\n",
       " 317057.7169972574,\n",
       " 452830.35308571986,\n",
       " 301926.1998551529,\n",
       " 336009.78920899256,\n",
       " 346677.97948669514,\n",
       " 387285.9873879991,\n",
       " 446713.5551442208,\n",
       " 616947.4131447615,\n",
       " 509350.8991154507,\n",
       " 335315.96648042847,\n",
       " 750554.1625089672,\n",
       " 646066.6894958322,\n",
       " 726019.0945194182,\n",
       " 349828.8182719596,\n",
       " 425445.0158270608,\n",
       " 653450.2620578167,\n",
       " 1808731.9089221717,\n",
       " 494938.3696690085,\n",
       " 685572.3298184727,\n",
       " 369571.2650985477,\n",
       " 372312.0807401157,\n",
       " 343697.4127513224,\n",
       " 687292.1048959712,\n",
       " 477395.6873700413,\n",
       " 317529.29956615175,\n",
       " 526493.6387674299,\n",
       " 348854.2340994795,\n",
       " 567665.235062789,\n",
       " 795450.845474513,\n",
       " 1058454.3724089493,\n",
       " 328773.20169900503,\n",
       " 537186.900998062,\n",
       " 1220882.2593502058,\n",
       " 464068.45398931555,\n",
       " 813812.6566930756,\n",
       " 625602.7713512279,\n",
       " 460512.4192994776,\n",
       " 816314.4593424859,\n",
       " 713344.5402934038,\n",
       " 255077.94526336194,\n",
       " 374432.0939745389,\n",
       " 285752.3079485908,\n",
       " 434811.12571711803,\n",
       " 526209.2833331347,\n",
       " 337736.57241390034,\n",
       " 499541.9402026294,\n",
       " 535477.9015061273,\n",
       " 415255.97778096225,\n",
       " 500622.4939363666,\n",
       " 356536.49094776675,\n",
       " 833240.4266861366,\n",
       " 343032.14952515386,\n",
       " 372593.76501667436,\n",
       " 374570.6138184626,\n",
       " 1184250.2122928074,\n",
       " 462125.50939025945,\n",
       " 285707.7072962841,\n",
       " 285565.72872198484,\n",
       " 464143.7113702321,\n",
       " 823351.0780585031,\n",
       " 475355.8764007246,\n",
       " 673280.5943579177,\n",
       " 543114.9689689585,\n",
       " 449290.59455334296,\n",
       " 990509.1439664403,\n",
       " 280377.6938933737,\n",
       " 905236.8326513555,\n",
       " 635814.6526128099,\n",
       " 518528.7663491281,\n",
       " 1031457.5235232443,\n",
       " 471389.02810845134,\n",
       " 278785.9160214059,\n",
       " 844587.4940268085,\n",
       " 338936.36026351637,\n",
       " 629173.3418997141,\n",
       " 1436067.6475647776,\n",
       " 345907.76886005583,\n",
       " 499903.05726157135,\n",
       " 449370.60946591204,\n",
       " 368284.2100723133,\n",
       " 254854.26952673044,\n",
       " 875931.065027061,\n",
       " 437982.2955078652,\n",
       " 286429.20046729536,\n",
       " 473969.8948995168,\n",
       " 354738.12035456486,\n",
       " 284007.2071990668,\n",
       " 492702.54319486703,\n",
       " 357495.09308677964,\n",
       " 414497.63409376086,\n",
       " 394125.9027324894,\n",
       " 478758.2148234135,\n",
       " 486167.5075283226,\n",
       " 549130.976183154,\n",
       " 781598.7922648514,\n",
       " 333774.59153060016,\n",
       " 329188.61307770107,\n",
       " 384033.0028791854,\n",
       " 384333.351298599,\n",
       " 329462.81921582465,\n",
       " 418810.5086517755,\n",
       " 771971.9769032696,\n",
       " 498075.05196468707,\n",
       " 842898.080072006,\n",
       " 838790.2649974378,\n",
       " 477743.0826496743,\n",
       " 440156.8934535816,\n",
       " 479770.6891873813,\n",
       " 313923.26077527704,\n",
       " 402417.62601570674,\n",
       " 441901.45767744514,\n",
       " 316826.89277539845,\n",
       " 472592.10693341313,\n",
       " 673280.5943579177,\n",
       " 417110.54961047013,\n",
       " 406654.14619054704,\n",
       " 462254.0880578054,\n",
       " 413740.87470442394,\n",
       " 1013140.5130974877,\n",
       " 763688.5249845529,\n",
       " 753493.5279194039,\n",
       " 366830.0777383891,\n",
       " 459041.2994871707,\n",
       " 402682.98732340866,\n",
       " 572612.9005784341,\n",
       " 265028.87911527505,\n",
       " 390952.8829639777,\n",
       " 355408.03948367835,\n",
       " 314309.6780890726,\n",
       " 358098.6133465591,\n",
       " 286976.29393883777,\n",
       " 414753.7818189061,\n",
       " 595242.2068657392,\n",
       " 309280.44858191937,\n",
       " 441899.7974742277,\n",
       " 513688.6206599328,\n",
       " 490156.7332842046,\n",
       " 348434.77896185353,\n",
       " 335841.7051891165,\n",
       " 286882.7253616686,\n",
       " 535360.3538913027,\n",
       " 472656.91559885244,\n",
       " 1707155.1501820202,\n",
       " 460512.4192994776,\n",
       " 2167728.8810612266,\n",
       " 676337.4156755179,\n",
       " 1035102.0425240115,\n",
       " 614153.2838332547,\n",
       " 416966.2073443992,\n",
       " 456384.96203757357,\n",
       " 994317.7791802675,\n",
       " 694425.0684454043,\n",
       " 478185.49775325763,\n",
       " 474576.5141438968,\n",
       " 754648.1090384696,\n",
       " 502259.61715414416,\n",
       " 386759.3687549274,\n",
       " 407422.32608866185,\n",
       " 363390.06882871833,\n",
       " 469611.97247420123,\n",
       " 334099.7670582257,\n",
       " 719693.4370572382,\n",
       " 509149.74184865877,\n",
       " 797511.6896380082,\n",
       " 695653.9706510574,\n",
       " 506087.54483742075,\n",
       " 447956.09395346086,\n",
       " 485117.1326212402,\n",
       " 492031.67348022235,\n",
       " 504406.61948220915,\n",
       " 557338.9300249546,\n",
       " 308457.6565631321,\n",
       " 1086106.2654171793,\n",
       " 311160.77203513624,\n",
       " 580506.3890319819,\n",
       " 277298.6345362169,\n",
       " 1736585.2676488722,\n",
       " 843410.4777159736,\n",
       " 629466.0207733907,\n",
       " 514074.0620946281,\n",
       " 642111.1957580708,\n",
       " 558871.5279789297,\n",
       " 349262.09756514226,\n",
       " 528482.4341134903,\n",
       " 464074.1970015198,\n",
       " 1003882.3000154783,\n",
       " 377708.1659774205,\n",
       " 504083.3387217289,\n",
       " 998302.9750641786,\n",
       " 530189.198391881,\n",
       " 456397.5907264001,\n",
       " 451125.6714482527,\n",
       " 1074507.7773221533,\n",
       " 317400.79780238046,\n",
       " 632047.7159180379,\n",
       " 790168.7524503377,\n",
       " 317400.79780238046,\n",
       " 625188.5182224675,\n",
       " 609196.0374950194,\n",
       " 398014.69977287273,\n",
       " 472045.385435475,\n",
       " 619763.5229530783,\n",
       " 432158.02780019474,\n",
       " 349828.8182719596,\n",
       " 531899.2429960732,\n",
       " 277298.6345362169,\n",
       " 349820.22403375286,\n",
       " 471091.23410172854,\n",
       " 529171.5386442734,\n",
       " 446921.7369862615,\n",
       " 812598.7047590198,\n",
       " 412266.0372764161,\n",
       " 299543.4574700815,\n",
       " 325688.98804702784,\n",
       " 470034.7983171948,\n",
       " 732091.2309765003,\n",
       " 751382.9330545391,\n",
       " 370847.7189264049,\n",
       " 291026.9345194457,\n",
       " 385018.16357161716,\n",
       " 272780.41462573706,\n",
       " 846652.8994466811,\n",
       " 403648.76938431954,\n",
       " 284705.92184044636,\n",
       " 338196.3564654657,\n",
       " 487278.27243814006,\n",
       " 490357.5111690277,\n",
       " 470034.7983171948,\n",
       " 1132750.4232176172,\n",
       " 1025440.2088322326,\n",
       " 1198241.3195022948,\n",
       " 434126.4884588643,\n",
       " 583869.2225300706,\n",
       " 366759.70038577757,\n",
       " 508955.31031927717,\n",
       " 358007.3561864828,\n",
       " 391400.7230227113,\n",
       " 347531.06407086615,\n",
       " 809658.960994756,\n",
       " 464447.9013170713,\n",
       " 359530.94876917306,\n",
       " 394887.2272954648,\n",
       " 294169.05993595574,\n",
       " 530880.1404972601,\n",
       " 481132.5137044866,\n",
       " 423912.55850597157,\n",
       " 526987.1394145717,\n",
       " 273360.81269499165,\n",
       " 635124.1497729906,\n",
       " 525634.603148159,\n",
       " 455178.116726312,\n",
       " 471776.6633007364,\n",
       " 299136.83107897366,\n",
       " 360914.56276539614,\n",
       " 1162143.6925436815,\n",
       " 757425.0127055324,\n",
       " 440277.15756107506,\n",
       " 318047.49027941166,\n",
       " 347998.41001492366,\n",
       " 1036082.5185795816,\n",
       " 553427.4429833541,\n",
       " 429872.21248072607,\n",
       " 277603.34285844857,\n",
       " 380575.32027595845,\n",
       " 295759.69463617785,\n",
       " 840375.237821791,\n",
       " 254322.17332972732,\n",
       " 323976.98780996836,\n",
       " 296804.2284770207,\n",
       " 468839.5617316934,\n",
       " 479109.2046411918,\n",
       " 591914.267644873,\n",
       " 581842.6206920042,\n",
       " 388116.7520670947,\n",
       " 473766.32262343756,\n",
       " 569625.3631156447,\n",
       " 485587.8715292337,\n",
       " 471395.371271968,\n",
       " 619444.6824427188,\n",
       " 510728.3396134607,\n",
       " 780165.488962341,\n",
       " 386692.5083158338,\n",
       " 321198.98221576004,\n",
       " 466615.0649669411,\n",
       " 591091.8958983121,\n",
       " 471410.42480440694,\n",
       " 494290.5292824548,\n",
       " 923254.8574508168,\n",
       " 525898.3642977925,\n",
       " 740724.5262555697,\n",
       " 623709.2464249341,\n",
       " 813590.6383515361,\n",
       " 277600.97701879515,\n",
       " 469228.23830764915,\n",
       " 470397.903056807,\n",
       " 816473.7310915317,\n",
       " 854745.4123775517,\n",
       " 271621.32852750615,\n",
       " 258338.40550182763,\n",
       " 490373.79860969004,\n",
       " 298475.0059249338,\n",
       " 704947.128942792,\n",
       " 391833.9975062452,\n",
       " 475513.2181380462,\n",
       " 513244.1863249622,\n",
       " 386422.2950368907,\n",
       " 296804.2284770207,\n",
       " 293826.10295684077,\n",
       " 321558.43058860215,\n",
       " 636500.1668018996,\n",
       " 286498.6140805,\n",
       " 370440.3735289626,\n",
       " 476348.68169378984,\n",
       " 390137.4677082725,\n",
       " 521555.488018499,\n",
       " 370016.81630407803,\n",
       " 391400.7230227113,\n",
       " 464447.9013170713,\n",
       " 352490.87235337385,\n",
       " 1199072.600677112,\n",
       " 618762.0385703888,\n",
       " 327655.0759641319,\n",
       " 456446.0352062459,\n",
       " 755059.9279459725,\n",
       " 635329.9514238606,\n",
       " 266810.8025596659,\n",
       " 469280.19517269696,\n",
       " 390587.71740241966,\n",
       " 299195.8609381948,\n",
       " 858714.3072184945,\n",
       " 473963.8714271026,\n",
       " 553793.8250286117,\n",
       " 485460.69538172625,\n",
       " 683940.9817851557,\n",
       " 582403.4718039696,\n",
       " 841429.5352796157,\n",
       " 654934.0255444667,\n",
       " 793298.1278248766,\n",
       " 381694.60189486644,\n",
       " 1013487.4523861568,\n",
       " 621871.7451004495,\n",
       " 772898.3327814757,\n",
       " 463505.47083946684,\n",
       " 466725.73228311783,\n",
       " 379941.2086859786,\n",
       " 464447.9013170713,\n",
       " 374432.0939745389,\n",
       " 782896.0438927094,\n",
       " 527145.3308414706,\n",
       " 487438.1930877526,\n",
       " 278346.7841585943,\n",
       " 277298.6345362169,\n",
       " 299359.5597346405,\n",
       " 806089.5517752175,\n",
       " 647160.9844931625,\n",
       " 460878.6649295356,\n",
       " 360959.7295692295,\n",
       " 467521.947915938,\n",
       " 367777.13416693726,\n",
       " 651064.2417228823,\n",
       " 277523.0436771729,\n",
       " 646791.7999150094,\n",
       " 522750.5786101243,\n",
       " 352157.39940625697,\n",
       " 279118.89280642074,\n",
       " 485127.6998785873,\n",
       " 505972.03443758533,\n",
       " 377416.950844228,\n",
       " 688288.6274770541,\n",
       " 336009.78920899256,\n",
       " 391833.9975062452,\n",
       " 546572.4713067877,\n",
       " 301140.10478243895,\n",
       " 576254.8002127643,\n",
       " 686825.7107285267,\n",
       " 698820.0522809307,\n",
       " 490925.3382976513,\n",
       " 423493.1452649014,\n",
       " 694417.6761774396,\n",
       " 509239.33362822037,\n",
       " 647160.9844931625,\n",
       " 1533912.076975647,\n",
       " 543858.0589906093,\n",
       " 342567.4904946678,\n",
       " 695694.7379697064,\n",
       " 826677.1185454201,\n",
       " 342558.41367593297,\n",
       " 404177.56769173255,\n",
       " 376648.31538906775,\n",
       " 294516.7545882618,\n",
       " 512462.9497792342,\n",
       " 503358.11174384574,\n",
       " 408138.5171182684,\n",
       " 490576.5947309284,\n",
       " 327731.5913530932,\n",
       " 368991.78719734,\n",
       " 255351.9398961676,\n",
       " 348523.96533419384,\n",
       " 345243.84957498935,\n",
       " 331496.34655551816,\n",
       " 344174.2377429667,\n",
       " 333725.5415603501,\n",
       " 804748.2704567349,\n",
       " 420238.2656773863,\n",
       " 438599.83900516015,\n",
       " 712879.0052122417,\n",
       " 355289.5757161694,\n",
       " 254322.17332972732,\n",
       " 368030.14453275723,\n",
       " 294555.6147583011,\n",
       " 579728.0206252061,\n",
       " 301605.712679647,\n",
       " 517626.7806054943,\n",
       " 813906.1433626346,\n",
       " 573841.8622478152,\n",
       " 558781.2704279319,\n",
       " 501947.96481771837,\n",
       " 268811.34679061046,\n",
       " 949008.9765194481,\n",
       " 638513.0614077034,\n",
       " 868577.9659306373,\n",
       " 326826.38269555714,\n",
       " 464448.49779345404,\n",
       " 808395.0612073779,\n",
       " 511930.9304103599,\n",
       " 256849.22747779198,\n",
       " 1157643.6477265458,\n",
       " 311547.62273313693,\n",
       " 633183.4640649951,\n",
       " 473969.8948995168,\n",
       " 741889.5993957157,\n",
       " 559524.1139999569,\n",
       " 452803.4942691295,\n",
       " 459041.2994871707,\n",
       " 443987.7332533566,\n",
       " 550522.877606167,\n",
       " 1298233.8002175444,\n",
       " 474273.23311753024,\n",
       " 834882.4343516583,\n",
       " 484115.77192416386,\n",
       " 382327.41465804627,\n",
       " 276251.38709437434,\n",
       " 432591.81816240714,\n",
       " 558951.6391690592,\n",
       " 488797.5244005374,\n",
       " 453027.88164286135,\n",
       " 295759.69463617785,\n",
       " 362530.1215014521,\n",
       " 351140.7223149899,\n",
       " 506846.13703397085,\n",
       " 558200.1991826714,\n",
       " 285944.3899666555,\n",
       " 382887.41214241274,\n",
       " 506923.36161338794,\n",
       " 625188.5182224675,\n",
       " 978419.7252258842,\n",
       " 661410.2243286421,\n",
       " 360126.73448154575,\n",
       " 783401.8766601175,\n",
       " 715946.5368504885,\n",
       " 848974.7592537845,\n",
       " 349904.1222265903,\n",
       " 2303091.4030317795,\n",
       " 369817.6170138207,\n",
       " 380619.32082605996,\n",
       " 507418.705427932,\n",
       " 332681.3962521,\n",
       " 354052.15903532825,\n",
       " 552024.6993656467,\n",
       " 660869.9300422005,\n",
       " 713854.2689911933,\n",
       " 277235.81475093827,\n",
       " 354395.7334257701,\n",
       " 553525.3408959043,\n",
       " 2270129.126521514,\n",
       " 555610.155661737,\n",
       " 465724.202364298,\n",
       " 915235.2344075321,\n",
       " 625447.5542417096,\n",
       " 555189.9902446261,\n",
       " 815541.272582798,\n",
       " 724886.3268765454,\n",
       " 452803.4942691295,\n",
       " 294280.7814130777,\n",
       " 441329.5569523336,\n",
       " 650987.4030897033,\n",
       " 556049.8528959684,\n",
       " 284046.6682063264,\n",
       " 299171.7248939816,\n",
       " 565493.7288928857,\n",
       " 413938.2818250021,\n",
       " 673596.9689622854,\n",
       " 490344.14208136004,\n",
       " 945247.8668065963,\n",
       " 380234.01653541747,\n",
       " 293837.4305343506,\n",
       " 394533.0224071256,\n",
       " 289548.43198969756,\n",
       " 360878.35947213613,\n",
       " 342558.41367593297,\n",
       " 393411.4704633829,\n",
       " 901759.926952018,\n",
       " 585789.459823308,\n",
       " 362446.1132255437,\n",
       " 509370.9424799258,\n",
       " 656443.8337730181,\n",
       " 256754.4200110001,\n",
       " 534090.4341158267,\n",
       " 367992.72841225605,\n",
       " 641428.4310153895,\n",
       " 638628.4411939302,\n",
       " 463230.304061033,\n",
       " 290456.99827496323,\n",
       " 435491.4839777809,\n",
       " 351273.42805578106,\n",
       " 884845.9993751261,\n",
       " 359047.2458480727,\n",
       " 567906.6380548866,\n",
       " 411535.3260140821,\n",
       " 388886.8181086009,\n",
       " 350775.100037019,\n",
       " 344315.73618414585,\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209949.74193651555"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqrt(mean_squared_error(bagging_predict,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
